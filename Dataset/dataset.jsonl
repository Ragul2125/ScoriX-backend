{
  "image": "IMG-20251026-WA0259.jpg",
  "ocr_text": "5. Layer types of Convolutional Layer (CNN): In Convolutional Neural Networks, it consists of several layers which classify an image and its parts. The common layers are: -> Input layer -> Convolutional layer -> Dropout layer -> Dense layer -> Pooling layer -> Output layer. Part-C. 12. Building Blocks of Convolutional Neural Networks: Convolutional Neural Networks are used to classify the image recognition, speech processing and natural language processing. * It is based on the Convolutional layer which consists of all layers and is fully connected to the Neural Network. It gives the more valuable features for the prediction. * Convolution Neural Network building blocks is defined as to extract the features from the Data.",
  "json_output": "[{\"question_number\":5,\"question_text\":\"Layer types of Convolutional Layer (CNN)\",\"answer_text\":\"In Convolutional Neural Networks, it consists of several layers which classify an image and its parts. The common layers are:\\n-> Input layer\\n-> Convolutional layer\\n-> Dropout layer\\n-> Dense layer\\n-> Pooling layer\\n-> Output layer\"},{\"question_number\":12,\"question_text\":\"Building Blocks of Convolutional Neural Networks (Part-C)\",\"answer_text\":\"Convolutional Neural Networks are used to classify the image recognition, speech processing and natural language processing.\\n\\n* It is based on the Convolutional layer which consists of all layers and is fully connected to the Neural Network. It gives the more valuable features for the prediction.\\n* Convolution Neural Network building blocks is defined as to extract the features from the Data.\"}]"
}
{
  "image": "IMG-20251026-WA0263.jpg",
  "ocr_text": "17. Activation function: An Activation function is a mathematical function that is applied after the weighted sum of input data and improves the quality of the model. The activation function activates when the threshold crosses a value. Types: 1. Binary Activation function 2. Linear Activation function 3. Non-Linear Activation function. 1. Binary Activation Function: * Binary Activation Function which gives either 0 or 1 based on the threshold value and it can be applied to classification problem. ",
  "json_output": "[{\"question_number\":17,\"question_text\":\"Activation function\",\"answer_text\":\"An Activation function is a mathematical function that is applied after the weighted sum of input data and improves the quality of the model. The activation function activates when the threshold crosses a value.\\n\\nTypes:\\n1. Binary Activation function\\n2. Linear Activation function\\n3. Non-Linear Activation function\\n\\n1. Binary Activation Function:\\n* Binary Activation Function which gives either 0 or 1 based on the threshold value and it can be applied to classification problem.\\n\"}]"
}

{
  "image": "IMG-20251026-WA0262.jpg",
  "ocr_text": "Convolutional layer: * Convolutional layer which extracts the features by the weighted sum method and it will give the predicted value. It separates the features from the model; the model's prediction is based on the Convolution. * The layer of the Building blocks of Convolutional Neural Networks which should handle the performance based prediction. It applied the mathematical solutions to figure out the problem. * The following properties are considered as the Building blocks of Convolutional Neural Networks. ",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Convolutional Layer and Building Blocks\",\"answer_text\":\"Convolutional layer:\\n* Convolutional layer which extracts the features by the weighted sum method and it will give the predicted value. It separates the features from the model; the model's prediction is based on the Convolution.\\n\\nBuilding Blocks:\\n* The layer of the Building blocks of Convolutional Neural Networks which should handle the performance based prediction. It applied the mathematical solutions to figure out the problem.\\n* The following properties are considered as the Building blocks of Convolutional Neural Networks.\\n\\n\"}]"
}

{
  "image": "IMG-20251026-WA0254.jpg",
  "ocr_text": " * Therefore this will be the final structure that CNN will use to work smoothly and efficiently. Input layer: gets the input from the image. Pooling: will reduce the size of the image resolution. Feature Extraction: reduces the features.",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"CNN Architecture and Layer Functions (Diagram)\",\"answer_text\":\"\\n\\n* Therefore this will be the final structure that CNN will use to work smoothly and efficiently.\\n\\nInput layer: gets the input from the image.\\nPooling: will reduce the size of the image resolution.\\nFeature Extraction: reduces the features.\"}]"
}

{
  "image": "IMG-20251026-WA0261.jpg",
  "ocr_text": "* In Convolutional Neural Network it passes through the pooling layer and it reduces the spatial size (width and length) of the image. It gives the most important spatial information. * It converts the prediction based on the grid box method. Convolutional Neural Networks are based on an available model for the image and series based model. * Which converts it into a grid and makes the prediction. Pooling Layer: * Pooling layer which is based on reducing the spatial dimensions [length and width]. It gives the important information from the features of a Deep Neural Network. * It can be separated into the values based on the following: Max pooling, Average pooling, and Global pooling. Everything is only based on the layer.",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Working of Pooling Layer (Cont.)\",\"answer_text\":\"* In Convolutional Neural Network it passes through the pooling layer and it reduces the spatial size (width and length) of the image. It gives the most important spatial information.\\n* It converts the prediction based on the grid box method. Convolutional Neural Networks are based on an available model for the image and series based model.\\n* Which converts it into a grid and makes the prediction.\\n\\nPooling Layer:\\n* Pooling layer which is based on reducing the spatial dimensions [length and width]. It gives the important information from the features of a Deep Neural Network.\\n* It can be separated into the values based on the following: Max pooling, Average pooling, and Global pooling. Everything is only based on the layer.\"}]"
}

{
  "image": "IMG-20251026-WA0260.jpg",
  "ocr_text": "Input layer, Dense layer, Output layer. * Input layer passes the weighted values to the Dense layer. It extracts the required features to describe the process. * After the Convolutional layer should be added in between the Dense layer and Input layer. And the activation should not be active when it does not reach the threshold (including autoencoder). ",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Layers of a simple Neural Network (Illustrated)\",\"answer_text\":\"Input layer, Dense layer, Output layer.\\n\\n* Input layer passes the weighted values to the Dense layer. It extracts the required features to describe the process.\\n* The Convolutional layer should be added in between the Dense layer and Input layer. And the activation should not be active when it does not reach the threshold (including autoencoder).\\n\\nInput layer -> Convolution layer -> Dense layer -> Output layer\\n\"}]"
}

{
  "image": "IMG-20251026-WA0255.jpg",
  "ocr_text": "8. Hyper Parameter: * Hyper Parameters include: * Learning rate * Momentum * Sparsity. Learning rate: * Manages how the weights are adjusted (manages how fast the model learns).",
  "json_output": "[{\"question_number\":8,\"question_text\":\"Hyper Parameter\",\"answer_text\":\"Hyper Parameters include:\\n* Learning rate\\n* Momentum\\n* Sparsity\\n\\nLearning rate:\\n* Manages how the weights are adjusted (manages how fast the model learns).\"}]"
}

{
  "image": "IMG-20251026-WA0258.jpg",
  "ocr_text": "3. Non-Linear Activation Function: * Non-Linear Activation Function handles more independent variables as compared to Linear Activation Function. It consists of Robust to avoid the correlation. Types: -> Sigmoid -> Tanh -> ReLu -> SoftMax. 4. Pooling layer: * Pooling layer is defined as to reduce the Spatial Dimensions [length and width]. It gives the important information from the features in a Deep Neural Network. Types of Pooling: -> Max pooling -> Average pooling -> Global pooling.",
  "json_output": "[{\"question_number\":3,\"question_text\":\"Non-Linear Activation Function\",\"answer_text\":\"* Non-Linear Activation Function handles more independent variables as compared to Linear Activation Function. It consists of Robust to avoid the correlation.\\n\\nTypes:\\n-> Sigmoid\\n-> Tanh\\n-> ReLu\\n-> SoftMax\"},{\"question_number\":4,\"question_text\":\"Pooling layer\",\"answer_text\":\"* Pooling layer is defined as to reduce the Spatial Dimensions [length and width]. It gives the important information from the features in a Deep Neural Network.\\n\\nTypes of Pooling:\\n-> Max pooling\\n-> Average pooling\\n-> Global pooling\"}]"
}

{
  "image": "IMG-20251026-WA0257.jpg",
  "ocr_text": "Part-A. 1. Perceptron: A Neural Network consists of a single input layer and fully connected perceptron which is called the Threshold Logic Unit [TLU]. Perceptron Consists of: -> Input layer or Input nodes -> Bias and weighted sum -> Net sum -> Activation function.  2. Vanishing gradient: A vanishing gradient is defined as a problem occurs in backpropagation which the values are back small values. It leads to slow or stops learning in Deep network.",
  "json_output": "[{\"question_number\":1,\"question_text\":\"Perceptron\",\"answer_text\":\"A Neural Network consists of a single input layer and a fully connected perceptron which is called the Threshold Logic Unit (TLU).\\nPerceptron Consists of:\\n-> Input layer or Input nodes\\n-> Bias and weighted sum\\n-> Net sum\\n-> Activation function\\n\"},{\"question_number\":2,\"question_text\":\"Vanishing gradient\",\"answer_text\":\"A vanishing gradient is defined as a problem that occurs in backpropagation where the values are very small.\\nIt leads to slow or stops learning in Deep Networks.\"}]"
}

{
  "image": "IMG-20251022-WA0109.jpg",
  "ocr_text": "7. Loss function: Measures errors between actual and predicted output. (i) Classification: * Cross Entropy Loss L = - Σi yi log(pi) * Used for multi-class classification (Softmax output). (ii) Regression: * Mean Squared Error (MSE) MSE = 1/n Σi (yi - ŷi)²",
  "json_output": "[{\"question_number\":7,\"question_text\":\"Loss function\",\"answer_text\":\"Measures errors between actual and predicted output.\\n\\n(i) Classification:\\n* Cross Entropy Loss\\n$$L = - \\sum_{i} y_i \\log(p_i)$$\\n* Used for multi-class classification (Softmax output).\\n\\n(ii) Regression:\\n* Mean Squared Error (MSE)\\n$$MSE = \\frac{1}{n} \\sum_{i} (y_i - \\hat{y}_i)^2$$"}]"
}
{
  "image": "IMG-20251026-WA0272.jpg",
  "ocr_text": "2. Randomized CV search: * The Randomized CV search method is used to enumerate the search between the following the searching algorithm and it has... 6. Fundamental principle of neural Network: * The Neural Network which is commonly by every neuron in the Neural network is connected both other and it consists of one Input layer and more than one Hidden layer. * Every signals passing through by the process of the node.",
  "json_output": "[{\"question_number\":2,\"question_text\":\"Randomized CV Search\",\"answer_text\":\"The **Randomized CV search method** is used to enumerate the search between the following the searching algorithm and it has...\"},{\"question_number\":6,\"question_text\":\"Fundamental principle of neural Network\",\"answer_text\":\"The **Neural Network** which is commonly connected by every neuron in the Neural network and it consists of one **Input layer** and more than one **Hidden layer**.\\n* Every signals passing through by the process of the node.\"}]"
}
{
  "image": "IMG-20251026-WA0266.jpg",
  "ocr_text": "[Graph showing Sigmoid and Tanh functions] iii) SoftMax: * Softmax which is defined as getting the maximum values of 0 to 1 and it can be calculated from values. * f(x) = Max (0, 1 - y * f(x)). iv) ReLu: * Regression Logic Unit which is defined as the ReLu which is more efficient activation function and commonly used: * f(x) = ex / (Σ ex). [Graph showing ReLu function]",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"SoftMax Activation Function\",\"answer_text\":\"* **Softmax** which is defined as getting the maximum values of 0 to 1 and it can be calculated from values. * $f(x) = \\text{Max} (0, 1 - y \\cdot f(x))$.\"},{\"question_number\":\"undefined\",\"question_text\":\"ReLu Activation Function\",\"answer_text\":\"* **Regression Logic Unit** which is defined as the **ReLu** which is a more efficient activation function and commonly used.\\n* $f(x) = \\frac{e^x}{\\sum e^x}$.\\n[Graph showing ReLu function]\"}]"
}


{
  "image": "IMG-20251026-WA0269.jpg",
  "ocr_text": "7. Loss function: * Loss function is defined as it is a mathematical concept to improve the quality of the data. Types: 1. Regression loss function 2. Classification loss function. 1. Regression loss function: * Regression loss function to calculate the loss value for the continuous variables to avoid Robust. Types: (i) MSE: * MSE [Mean Squared Error] which is difference of actual and predicted value. Square it and average the results is a MSE. * $MSE = 1 / 2M \\sum_i (\\hat{y}_i - y_i)^2$. * $MSE = 1 / 2M \\sum_i (\\hat{y}_i - y_i).$",
  "json_output": "[{\"question_number\":7,\"question_text\":\"Loss function\",\"answer_text\":\"* **Loss function** is defined as a mathematical concept to improve the quality of the data.\\nTypes:\\n1. Regression loss function\\n2. Classification loss function.\\n\\n1. **Regression loss function**:\\n* Regression loss function is to calculate the loss value for the **continuous variables** to avoid Robust.\\nTypes:\\n(i) **MSE**:\\n* **MSE (Mean Squared Error)** which is the difference between actual and predicted value. Square it and average the results is an MSE.\\n* $MSE = \\frac{1}{2M} \\sum_{i} (\\hat{y}_i - y_i)^2$.\"}]"
}

{
  "image": "IMG-20251026-WA0264.jpg",
  "ocr_text": "2. Linear Activation Function: * Linear Activation function which is used by regression based algorithm which consists of one independent variable and more than one dependent variable. It ranges from -∞ to +∞. [Graph showing Linear Activation Function] 3. Non-Linear Activation function: * Non-Linear Activation function which is used for both classification and regression problem and it consists of complicated formula. * It is used for classification (discrete variable [Male, Female]) or continuous values (Salary, Age).",
  "json_output": "[{\"question_number\":2,\"question_text\":\"Linear Activation Function\",\"answer_text\":\"* **Linear Activation function** is used by regression based algorithm which consists of one **independent variable** and more than one **dependent variable**. It ranges from $-\\infty$ to $+\\infty$. [Graph showing Linear Activation Function]\"},{\"question_number\":3,\"question_text\":\"Non-Linear Activation function\",\"answer_text\":\"* **Non-Linear Activation function** is used for both **classification** and **regression** problem and it consists of a complicated formula.\\n* It is used for classification (discrete variable [Male, Female]) or continuous values (Salary, Age).\"}]"
}

{
  "image": "IMG-20251026-WA0268.jpg",
  "ocr_text": "[Image showing an input matrix and a filter/window operation] Average pooling: * Average pooling is defined as getting the average of the all values in the different features. * It reduces all the values by skipping movement. [Image showing Average Pooling matrices and output]",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Average pooling\",\"answer_text\":\"* **Average pooling** is defined as getting the **average** of all values in the different features.\\n* It reduces all the values by skipping movement.\\n[Image showing Average Pooling matrices and output]\"}]"
}
{
  "image": "IMG-20251026-WA0270.jpg",
  "ocr_text": "(ii) MAE: * MAE [Mean absolute error] for the difference of actual value to the predicted value and average it. * $MAE = 1 / M \\sum_{i=1}^{M} |\\hat{y}_i - y_i|$. (iii) Huber Loss Function: * Huber Loss Function is in between the MSE and MAE. * $H(a, b) = 1 / 2M \\sum_{i} |\\hat{y}_i - y_i|^2 ...$ Loss Function for Classification is based on Binary Entropy loss Function: * Binary Entropy loss Function which is used for the **classification problem**. * $BF = 1 / 2M ...$ * $BF = -1 / M (\\hat{y} \\cdot \\log(y)) - 1 / M (1-\\hat{y}) + \\log(1-y)$",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Mean Absolute Error (MAE)\",\"answer_text\":\"* **MAE (Mean absolute error)** is the difference of actual value to the predicted value and average it.\\n* $MAE = \\frac{1}{M} \\sum_{i=1}^{M} |\\hat{y}_i - y_i|. \"},{\"question_number\":\"undefined\",\"question_text\":\"Huber Loss Function\",\"answer_text\":\"* **Huber Loss Function** is in between the **MSE** and **MAE**.\\n* $H(a, b) = \\frac{1}{2M} \\sum_{i} |\\hat{y}_i - y_i|^2 ...\"},{\"question_number\":\"undefined\",\"question_text\":\"Binary Entropy Loss Function (Classification)\",\"answer_text\":\"* **Binary Entropy loss Function** is used for the **classification problem**.\\n* $BF = -\\frac{1}{M} (\\hat{y} \\cdot \\log(y)) - \\frac{1}{M} (1-\\hat{y}) \\cdot \\log(1-y)$\"}]"
}
{
  "image": "IMG-20251026-WA0267.jpg",
  "ocr_text": "Part-B. 9. Pooling layer: * Pooling layer in the convolutional neural network is defined as to reduce the Spatial dimensions [length and width] to retain the important information from the features. * It reduces the length and width from the filter separated channel based on the formula: ($[ (nw - f + 1) / s ] \\times nc$). Types: 1. Max pooling 2. Global pooling 3. Average pooling. 1. Max pooling: * To filter the dimension based on the maximum values to calculate and it will added to the channels. * Maximum value is added to the new grid to reduce other values.",
  "json_output": "[{\"question_number\":9,\"question_text\":\"Pooling layer\",\"answer_text\":\"* **Pooling layer** in the **convolutional neural network** is defined as to **reduce the Spatial dimensions [length and width]** to retain the important information from the features.\\n* It reduces the length and width from the filter separated channel based on the formula: $ [ \\frac{(n_w - f + 1)}{s} ] \\times n_c $ (This formula is often used for the output size, where $n_w$ is input width, $f$ is filter size, $s$ is stride, $n_c$ is number of channels, but the written form $ (n_w - f + 1) / s $ is for one dimension, assuming integer division).\"},{\"question_number\":\"undefined\",\"question_text\":\"Types of Pooling and Max Pooling\",\"answer_text\":\"Types:\\n1. Max pooling\\n2. Global pooling\\n3. Average pooling\\n\\n1. **Max pooling**:\\n* To filter the dimension based on the **maximum values** to calculate and it will be added to the channels.\\n* Maximum value is added to the new grid to reduce other values.\"}]"
}
{
  "image": "IMG-20251026-WA0265.jpg",
  "ocr_text": "Types: -> Sigmoid -> Tanh -> ReLu -> SoftMax. i) Sigmoid: * Sigmoid Activation the values are ranges from 0 to 1. The difference of negative infinity to positive 1. To reduce the robust from the model and it will describe it. * $f(x) = 1 / (1 + e^{-x})$. ii) Tanh: * Tanh Activation function: The values are ranges from -∞ to +∞ which is based on the Trignometric formulas and it can be calculated. * $f(x) = (e^x - e^{-x}) / (e^x + e^{-x})$.",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Function Types and Sigmoid\",\"answer_text\":\"Types:\\n-> Sigmoid\\n-> Tanh\\n-> ReLu\\n-> SoftMax\\n\\ni) **Sigmoid**:\\n* **Sigmoid Activation** values range from **0 to 1**. The difference is from negative infinity to positive 1. To reduce the robust from the model and it will describe it.\\n* $f(x) = \\frac{1}{1 + e^{-x}}. \"},{\"question_number\":\"undefined\",\"question_text\":\"Tanh Activation Function\",\"answer_text\":\"ii) **Tanh Activation function**:\\n* The values range from $-\\infty$ to $+\\infty$ which is based on the Trigonometric formulas and it can be calculated.\\n* $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}. \"}]"
}
{
  "image": "IMG-20251026-WA0273.jpg",
  "ocr_text": "[Diagram of an input layer, net sum, activation, and output layer: Input layer (x1, x2) -> Net Sum (Σ) -> Activation -> Output (Y)]. Inspired from Biological Neuron: * Each neuron is connected through by every other neuron. Each electrical pulse is transmitted to every neuron. * Transmit the neuron one to another by the electrical pulse and it leads to make the model decision and it can combine. [Text on another topic: Transmission, decision, combination]",
  "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Biological Neuron (Inspiration for NN)\",\"answer_text\":\"[Diagram of an input layer, net sum, activation, and output layer: Input layer ($x_1$, $x_2$) -> Net Sum ($\\Sigma$) -> Activation -> Output (Y)].\\n\\nInspired from **Biological Neuron**:\\n* Each **neuron** is connected to every other neuron. Each **electrical pulse** is transmitted to every neuron.\\n* Neurons transmit to one another by the electrical pulse, which leads to the model decision and combination.\"}]"
}
{
  "image": "IMG-20251026-WA0271.jpg",
  "ocr_text": "8. Hyper Parameters: * Hyper parameters is a combination of the spatial size to improve the feature quality. It can be added the dimensions to the input before convolution. * The hyper parameter which works on the searching on the parameters to avoid model overfitting. Types: 1. Grid CV search 2. Randomized CV search 3. Global search. 1. Grid CV search: * Grid CV search is one of the hyper parameter parabolic way of the searching method. ",
  "json_output": "[{\"question_number\":8,\"question_text\":\"Hyper Parameters\",\"answer_text\":\"* **Hyper parameters** is a combination of the spatial size to improve the feature quality. It can add dimensions to the input before convolution.\\n* The hyper parameter works on searching the parameters to **avoid model overfitting**.\\n\\nTypes:\\n1. **Grid CV search**\\n2. **Randomized CV search**\\n3. **Global search**\\n\\n1. **Grid CV search**:\\n* Grid CV search is one of the hyper parameter **parabolic way** of the searching method.\\n\"}]"
}
{
    "image": "IMG-20251026-WA0277.jpg",
    "ocr_text": "Part-B. 6. Fundamental principle of Neural Network. * Neural networks consists of Input layer, Hidden layer layer and output layer. (i) Input layer: It takes inputs as the features. (ii) Hidden layer: It processes and calculates the weighted sum. (iii) Output layer: It makes the predictions out of the weighted sum. (iv) Loss function: Loss = Actual Value - Predicted Value. If there are more errors, they backpropagate.",
    "json_output": "[{\"question_number\":6,\"question_text\":\"Fundamental principle of Neural Network\",\"answer_text\":\"* Neural networks consists of **Input layer**, **Hidden layer**, and **Output layer**.\\n(i) **Input layer**: It takes inputs as the features.\\n(ii) **Hidden layer**: It processes and calculates the weighted sum.\\n(iii) **Output layer**: It makes the predictions out of the weighted sum.\\n(iv) **Loss function**: Loss = **Actual Value - Predicted Value**.\\nIf there are more errors, they **backpropagate**.\"}]"
  }
  {
    "image": "IMG-20251026-WA0276.jpg",
    "ocr_text": "4. POOLING LAYER: Pooling layer is used for computing downsampling and controlling for computing overfitting. Types: -> Average Pooling -> Maximum Pooling. 5. LAYER TYPES OF CONVOLUTIONAL NEURAL NETWORK: * Convolutional Neural Network is used for image processing. * Types: (i) Convolution layer (ii) Pooling layer (iii) Flatten layer (iv) Fully Connected layer (v) Output layer (vi) Dropout layer.",
    "json_output": "[{\"question_number\":4,\"question_text\":\"Pooling Layer\",\"answer_text\":\"**Pooling layer** is used for computing **downsampling** and controlling for computing **overfitting**.\\nTypes:\\n-> Average Pooling\\n-> Maximum Pooling\"},{\"question_number\":5,\"question_text\":\"Layer Types of Convolutional Neural Network\",\"answer_text\":\"* **Convolutional Neural Network** is used for **image processing**.\\n* Types:\\n(i) Convolution layer\\n(ii) Pooling layer\\n(iii) Flatten layer\\n(iv) Fully Connected layer\\n(v) Output layer\\n(vi) Dropout layer\"}]"
  }
  {
    "image": "IMG-20251026-WA0278.jpg",
    "ocr_text": "1. Actually neural networks are inspired by human brain. Neurons: * Transmits the input signal. Dendrites: * It receive the output. Axons: * They receive the signal. Likewise, Artificial Neural Network also... [Diagram showing Input layer, weights, Sum (Σ), Activation function, Output layer]. * Input layer sends features. The weighted sum is calculated and to the Activation function and finally the output is received.",
    "json_output": "[{\"question_number\":1,\"question_text\":\"Biological vs. Artificial Neural Networks\",\"answer_text\":\"* Actually neural networks are inspired by the **human brain**.\\n* **Neurons** transmit the input signal.\\n* **Dendrites** receive the output.\\n* **Axons** receive the signal.\\n\\n[Diagram showing Input layer, weights, Sum (Σ), Activation function, Output layer].\\n* The **Input layer** sends features. The **weighted sum** is calculated and sent to the **Activation function**, and finally the **output** is received.\"}]"
  }
  {
    "image": "IMG-20251026-WA0275.jpg",
    "ocr_text": "PART A. 1. Perceptron: Perceptron is the simplest artificial neural network. * The binary inputs are passed as inputs, the sums of weights are calculated, the results are passed to Activation function and gives the binary output as final. 2. Vanishing Gradient: Vanishing gradient occurs when the gradients are small in backpropagation, then weights are not adjusted and updated. Spatial dimensions. Padding pressures. 3. Non-Linear Activation function: In non-linear activation function solves complex non-linearity and finds features. -> Sigmoid (Hyperbolic tangent) -> Tanh -> ReLu -> Softmax.",
    "json_output": "[{\"question_number\":1,\"question_text\":\"Perceptron\",\"answer_text\":\"**Perceptron** is the **simplest artificial neural network**.\\n* The binary inputs are passed as inputs, the sums of weights are calculated, the results are passed to **Activation function** and gives the binary output as final.\"},{\"question_number\":2,\"question_text\":\"Vanishing Gradient\",\"answer_text\":\"**Vanishing gradient** occurs when the **gradients are small in backpropagation**, then weights are not adjusted and updated.\"},{\"question_number\":3,\"question_text\":\"Non-Linear Activation function\",\"answer_text\":\"In **non-linear activation function** solves complex non-linearity and finds features.\\n-> Sigmoid (Hyperbolic tangent)\\n-> Tanh\\n-> ReLu\\n-> Softmax.\"}]"
  }
  {
    "image": "IMG-20251026-WA0284.jpg",
    "ocr_text": "TYPES: -> Average Pooling: The average value of the weights has been found. -> Maximum Pooling: [Diagram of a 2x2 grid] Output [8, 12, 10] should be maximum chosen. Role: It extracts the features from the convolution layer and then they are classified finally. (i) Binary classification or Multi-class classification. So, these are the brief description about pooling layer.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Average and Maximum Pooling Types\",\"answer_text\":\"Types:\\n-> **Average Pooling**: The average value of the weights has been found.\\n-> **Maximum Pooling**: [Diagram of a 2x2 grid showing a maximum value selection] Output values should be the **maximum chosen**.\\nRole: It extracts the features from the convolution layer and then they are classified finally.\\n(i) **Binary classification** or **Multi-class classification**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Overall Pooling Description\",\"answer_text\":\"So, these are the brief description about **pooling layer**.\"}]"
  }
  {
    "image": "IMG-20251026-WA0279.jpg",
    "ocr_text": "7. Loss function for Regression (i) Mean Squared Error: * It is the simplest loss function. It is the difference of actual value and predicted value and they are squared once and finally averaged. $MSE = 1 / 2N \\sum (y_i - \\hat{y}_i)^2$. (ii) Mean absolute error: * It is the difference of actual value and the predicted value. $MAE = 1 / N \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$. (iii) Huber Loss: $H L = 1/N \\sum: (y_i - \\hat{y}_i)$ if $|y_i - \\hat{y}_i| \\le \\delta$. $H L = ...$ if $|y_i - \\hat{y}_i| > \\delta$. This is how loss function of Regression is been calculated.",
    "json_output": "[{\"question_number\":7,\"question_text\":\"Loss function for Regression\",\"answer_text\":\"(i) **Mean Squared Error (MSE)**:\\n* It is the simplest loss function. It is the difference of **actual value** and **predicted value**, squared and finally averaged. $$MSE = \\frac{1}{2N} \\sum (y_i - \\hat{y}_i)^2$$\\n(ii) **Mean absolute error (MAE)**:\\n* It is the difference of **actual value** and the **predicted value**. $$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\\n(iii) **Huber Loss**:\\n$$HL = \\frac{1}{N} \\sum: (y_i - \\hat{y}_i) \\quad \\text{if } |y_i - \\hat{y}_i| \\le \\delta$$\\n$$HL = ... \\quad \\text{if } |y_i - \\hat{y}_i| > \\delta$$\\nThis is how loss function of Regression is been calculated.\"}]"
  }
  {
    "image": "IMG-20251026-WA0280.jpg",
    "ocr_text": "Loss function of Classification (i) Binary Cross Entropy (BCE): For 2 class... $BCE = -1 / M \\sum_{i=1}^{M} [y_i \\cdot \\log(\\hat{y}_i) + (1-y_i) \\cdot \\log(1-\\hat{y}_i)]$. Loss of BCE is loss. Binary loss entropy. (ii) Hinge Loss: Hinge loss is mainly used in support vector machine. Hinge loss = (0, 1 - y * f(x)). (iii) Categorical Cross Entropy (CCE): For multi-class $CCE = -1 / M \\sum_{i=1}^{M} y_i \\cdot \\log(\\hat{y}_i)$. So, these are the methods to find the loss function of classification.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Loss function of Classification\",\"answer_text\":\"(i) **Binary Cross Entropy (BCE)**:\\nFor 2 class... $BCE = -\\frac{1}{M} \\sum_{i=1}^{M} [y_i \\cdot \\log(\\hat{y}_i) + (1-y_i) \\cdot \\log(1-\\hat{y}_i)]$. Loss of BCE is loss. Binary loss entropy.\\n\\n(ii) **Hinge Loss**:\\nHinge loss is mainly used in **support vector machine**.\\nHinge loss = $\\max(0, 1 - y \\cdot f(x))$.\\n\\n(iii) **Categorical Cross Entropy (CCE)**:\\nFor multi-class $CCE = -\\frac{1}{M} \\sum_{i=1}^{M} y_i \\cdot \\log(\\hat{y}_i)$.\\n\\nSo, these are the methods to find the loss function of classification.\"}]"
  }
  {
    "image": "IMG-20251026-WA0285.jpg",
    "ocr_text": "PART C. 10. NEURAL NETWORKS: * Artificial neural networks are inspired from the biological neural network. * They commonly have three layers in neural networks: Input Layer, Hidden Layer, Output Layer. Perceptron: * Perceptron neural network is the simplest artificial network. * Binary inputs are present into the network. The weights are moved to the activation function and the output is received as output.",
    "json_output": "[{\"question_number\":10,\"question_text\":\"Neural Networks and Perceptron\",\"answer_text\":\"* **Artificial neural networks** are inspired from the **biological neural network**.\\n* They commonly have **three layers** in neural networks: **Input Layer, Hidden Layer, Output Layer**.\\n\\n**Perceptron**:\\n* **Perceptron neural network** is the **simplest artificial network**.\\n* Binary inputs are present into the network. The weights are moved to the activation function and the output is received as output.\"}]"
  }
  {
    "image": "IMG-20251026-WA0283.jpg",
    "ocr_text": "9. POOLING LAYER: Pooling layer is used for controlling for computing downsampling and avoiding overfitting. [Diagram of Pooling Layer] -> The spatial size of the feature is reduced. -> Dimensionality reduction: The dimensions of the features are reduced here. -> Overfitting: If the weights are reduced then there are ...",
    "json_output": "[{\"question_number\":9,\"question_text\":\"Pooling Layer (Cont.)\",\"answer_text\":\"**Pooling layer** is used for controlling for computing **downsampling** and **avoiding overfitting**.\\n[Diagram of Pooling Layer]\\n\\n-> The **spatial size** of the feature is reduced.\\n-> **Dimensionality reduction**: The dimensions of the features are reduced here.\\n-> **Overfitting**: If the weights are reduced then there are...\"}]"
  }
  {
    "image": "IMG-20251026-WA0281.jpg",
    "ocr_text": "8. HYPER PARAMETER: (i) Learning Rate: Learning rate is considered to be the main hyperparameter. * Learning rate checks the weights that are been updated. $W_{new} = W_{old} - Lr \\cdot \\Delta L$ where $Lr$: Learning rate, $\\Delta L$: Loss function. (ii) Optimization: Optimization checks the efficiency of the training data. Stochastic gradient descent is used. (iii) Batch Size: Batch size is the batch size that is involved in the training of the factors.",
    "json_output": "[{\"question_number\":8,\"question_text\":\"Hyper Parameter\",\"answer_text\":\"(i) **Learning Rate**:\\n**Learning rate** is considered to be the **main hyperparameter**.\\n* Learning rate checks the weights that are been updated.\\n$$W_{new} = W_{old} - L_r \\cdot \\Delta L$$\\nwhere $L_r$: Learning rate, $\\Delta L$: Loss function.\\n\\n(ii) **Optimization**:\\n**Optimization** checks the efficiency of the training data. **Stochastic gradient descent** is used.\\n\\n(iii) **Batch Size**:\\n**Batch size** is the batch size that is involved in the training of the factors.\"}]"
  }
{
    "image": "IMG-20251026-WA0296.jpg",
    "ocr_text": "Disadv: * Can cause vanishing gradient problem. [Graph showing Tanh function]. iii) ReLu (Rectified Linear Unit): * It outputs the value in the range of 0 to ∞. * $ReLu = \\max(0, x)$. Adv: * It supports backpropagation and it is differentiable. * Reduces vanishing gradient problem. Disadv: * It is not zero centered.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Disadvantages of Tanh\",\"answer_text\":\"**Disadv**: * Can cause **vanishing gradient problem**.\\n[Graph showing Tanh function].\"},{\"question_number\":\"undefined\",\"question_text\":\"ReLu (Rectified Linear Unit) Activation function\",\"answer_text\":\"iii) **ReLu (Rectified Linear Unit)**:\\n* It outputs the value in the range of **0 to $\\infty$**.\\n* $ReLu = \\max(0, x)$.\\n**Adv**: * It supports **backpropagation** and it is **differentiable**.\\n* **Reduces vanishing gradient problem**.\\n**Disadv**: * It is **not zero centered**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0286.jpg",
    "ocr_text": "[Diagram of a single perceptron: Input (x1, x2, x3) -> Weights (w1, w2, w3) -> Sum (Σ) -> Activation function (f) -> Output]. Perceptron: $O = \\Sigma x_i w_i + bias$. (ii) Multi-layered: * Multi-layered neural network is also known as **multi layered perceptron**. * The inputs are given to the weights and net sum is calculated and is moved into activation function and the output is received.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Perceptron and Multi-Layered Network\",\"answer_text\":\"[Diagram of a single perceptron: Input ($x_1, x_2, x_3$) -> Weights ($w_1, w_2, w_3$) -> Sum ($\\Sigma$) -> Activation function ($f$) -> Output].\\n\\n**Perceptron**:\\n$$O = \\sum x_i w_i + \\text{bias}$$ \\n(ii) **Multi-layered**:\\n* **Multi-layered neural network** is also known as **multi-layered perceptron**.\\n* The inputs are given to the weights, and the net sum is calculated and is moved into the **activation function** and the output is received.\"}]"
  },
  {
    "image": "IMG-20251026-WA0294.jpg",
    "ocr_text": "Part C (10 Mark). 17. Activation Function: * Activation function introduce non-linearity into the neural network, which makes the neural network learn and understand complex patterns. Types: 1. Linear Activation Function. 2. Non-linear Activation Function. 1. Linear Activation Function: * Linear Activation Function simply converts $y$ to $x$. $y = x$. * It does perform well only for linear variables. * It does not perform well on non-linear variables. [Graph showing linear activation function $y=x$]",
    "json_output": "[{\"question_number\":17,\"question_text\":\"Activation Function\",\"answer_text\":\"* **Activation function** introduces **non-linearity** into the neural network, which makes the neural network learn and understand **complex patterns**.\\nTypes:\\n1. Linear Activation Function.\\n2. Non-linear Activation Function.\\n\\n1. **Linear Activation Function**:\\n* Linear Activation Function simply converts $y$ to $x$: $y = x$.\\n* It does perform well only for **linear variables**.\\n* It does not perform well on **non-linear variables**.\\n[Graph showing linear activation function $y=x$]\"}]"
  },
  {
    "image": "IMG-20251026-WA0288.jpg",
    "ocr_text": "Loss function: Actual value - Predicted Value. -> Back propagation: * The error is back propagated from Output layer to the Input layer. -> Weight updation: * Weights are updated. * After back propagation all the weights are updated. * So, these are the steps applied back propagation in model training. -> It plays a major role in neural networks. -> They are mostly preferred during the classification problems. -> So, these are the brief description about neural networks.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Loss Function and Back Propagation\",\"answer_text\":\"**Loss function**: Actual value - Predicted Value.\\n\\n-> **Back propagation**:\\n* The error is **back propagated** from **Output layer** to the **Input layer**.\\n\\n-> **Weight updation**:\\n* Weights are updated.\\n* After back propagation all the weights are updated.\\n* These are the steps applied during model training.\\n\\n-> Back propagation plays a **major role in neural networks**.\\n-> They are mostly preferred during **classification problems**.\\n-> These are the brief description about neural networks.\"}]"
  },
  {
    "image": "IMG-20251026-WA0292.jpg",
    "ocr_text": "-> Fully Connected layer: Here they check non-linearity using activation functions. (i) Sigmoid: Binary classification [-1, 1]. (ii) Tanh: Hidden layer [-1, 1]. (iii) ReLu: Hidden layer [0, ∞]. (iv) Softmax: Multi-class classification [0, 1]. -> Output Layer: (i) When output is based on 2 classes, use Sigmoid activation function. (ii) When output is based on multiple classes, use Softmax activation function. So, these are all the building blocks of Convolutional Network.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Fully Connected and Output Layers\",\"answer_text\":\"-> **Fully Connected layer**:\\n* Here they check **non-linearity** using activation functions.\\n(i) **Sigmoid**: Binary classification [0, 1] (corrected from [-1, 1])\\n(ii) **Tanh**: Hidden layer [-1, 1]\\n(iii) **ReLu**: Hidden layer [0, $\\infty$]\\n(iv) **Softmax**: Multi-class classification [0, 1]\\n\\n-> **Output Layer**:\\n(i) When output is based on **2 classes**, use **Sigmoid** activation function.\\n(ii) When output is based on **multiple classes**, use **Softmax** activation function.\\n\\nSo, these are all the building blocks of **Convolutional Network**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0289.jpg",
    "ocr_text": "12. Building Blocks of Convolutional Neural Network: * Convolutional neural network is a deep neural network. * It is mainly used for image processing. * It has several layers such as: (i) Convolutional layer (ii) Pooling layer (iii) Flatten Layer (iv) Fully connected layer (v) Output Layer (vi) Dropout Layer.",
    "json_output": "[{\"question_number\":12,\"question_text\":\"Building Blocks of Convolutional Neural Network\",\"answer_text\":\"* **Convolutional neural network** is a deep neural network.\\n* It is mainly used for **image processing**.\\n* It has several layers such as:\\n(i) Convolutional layer\\n(ii) Pooling layer\\n(iii) Flatten Layer\\n(iv) Fully connected layer\\n(v) Output Layer\\n(vi) Dropout Layer.\"}]"
  },
  {
    "image": "IMG-20251026-WA0291.jpg",
    "ocr_text": "To find Dot Product: Consider a 2x2 grid, it is being placed over it. [Matrix example for dot product]. Pooling Layer: (i) Pooling layer is used for down-sampling, avoid overfitting and computation. Types: Average pooling, Maximum Pooling.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Dot Product and Pooling Layer\",\"answer_text\":\"To find **Dot Product**: Consider a 2x2 grid, it is being placed over it.\\n[Matrix example for dot product].\\n\\n**Pooling Layer**:\\n(i) **Pooling layer** is used for **down-sampling**, to avoid **overfitting** and **computation**.\\nTypes:\\n* Average pooling\\n* Maximum Pooling\"}]"
  },
  {
    "image": "IMG-20251026-WA0295.jpg",
    "ocr_text": "Non-linear Activation function: i) Sigmoid Activation function: * It outputs the value in a range of 0 to 1. * Sig = 1 / (1 + e^-x). Adv: * Used for binary classification tasks. Disadv: * It is not zero centered. [Graph showing Sigmoid function]. ii) Tanh Activation function: * It outputs the value in a range of [-1, 1]. * Tanh = (e^x - e^-x) / (e^x + e^-x). Adv: * It is zero centered.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Sigmoid Activation function\",\"answer_text\":\"i) **Sigmoid Activation function**:\\n* It outputs the value in a range of **0 to 1**.\\n* $Sig = \\frac{1}{1 + e^{-x}}$.\\n**Adv**: * Used for **binary classification tasks**.\\n**Disadv**: * It is **not zero centered**.\\n[Graph showing Sigmoid function].\"},{\"question_number\":\"undefined\",\"question_text\":\"Tanh Activation function\",\"answer_text\":\"ii) **Tanh Activation function**:\\n* It outputs the value in a range of **[-1, 1]**.\\n* $Tanh = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\\n**Adv**: * It is **zero centered**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0287.jpg",
    "ocr_text": "Input layer: Sending features. Hidden layer: Processes weights. Output layer: Receives output. [Diagram of a multi-layer neural network with 2 hidden layers]. When there are more than one hidden layer, then it is a deep neural network. (iii) Back propagation: * It occurs when there is more error. Loss function should be calculated and back propagate.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Neural Network Layers and Deep Learning\",\"answer_text\":\"**Input layer**: Sending features.\\n**Hidden layer**: Processes weights.\\n**Output layer**: Receives output.\\n[Diagram of a multi-layer neural network with 2 hidden layers].\\n\\n* When there are **more than one hidden layer**, then it is a **deep neural network**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Back propagation (Cont.)\",\"answer_text\":\"(iii) **Back propagation**:\\n* It occurs when there is **more error**. **Loss function** should be calculated and **back propagate**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0290.jpg",
    "ocr_text": "[Diagram of a CNN architecture: Input Layer -> Convolution Layer -> Pooling Layer -> Fully Connected Layer -> Output Layer, labeled as Feature Extraction and Classification]. Building Block of CNN: Input layer: * The features are input here and sent to the convolution layer. Convolution layer:",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Building Block of CNN Architecture\",\"answer_text\":\"[Diagram of a CNN architecture: Input Layer -> Convolution Layer -> Pooling Layer -> Fully Connected Layer -> Output Layer, labeled as Feature Extraction and Classification].\\n\\nBuilding Block of CNN:\\n-> **Input layer**: * The features are input here and sent to the **convolution layer**.\\n-> **Convolution layer**: ...\"}]"
  }{
      "image": "IMG-20251026-WA0307.jpg",
      "ocr_text": "* which helps not to process all the values in the feature map. * It extracts only key features by max pooling. * It extracts only key pattern wherever it is located. e.g.: * In a human image, the human's ear is located at the bottom right of the image. * With the help of the pooling layer, we can easily recognizing the human's ear. Part A (10 Mark). 1. Perceptron: * It is the simplest form of neural network. * It takes input and weights and adds a function to get the output. 2. Vanishing gradient: * Vanishing gradient is a problem in the weights updating in the back while...",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Feature Extraction / Pooling Benefits\",\"answer_text\":\"* It helps not to process all the values in the feature map.\\n* It extracts only **key features** by **max pooling**.\\n* It extracts only **key patterns** wherever it is located.\\n* **Example**: In a human image, the ear is located at the bottom right. With the help of the **pooling layer**, we can easily recognize the ear.\"},{\"question_number\":1,\"question_text\":\"Perceptron\",\"answer_text\":\"* It is the **simplest form of neural network**.\\n* It takes **input and weights** and adds an **activation function** to get the output.\"},{\"question_number\":2,\"question_text\":\"Vanishing gradient\",\"answer_text\":\"* **Vanishing gradient** is a problem in the **weights updating** in the back...\"}]"
    },
    {
      "image": "IMG-20251026-WA0304.jpg",
      "ocr_text": "It controls how the network learns from the training data. Some of the hyperparameters are listed below. i) Learning Rate (α): * Determines how big a step a model can usually take while learning it's value usually be 0.1, 0.001, 0.01. * If learning rate is high, the model does not learn properly. * If learning rate is low, the model will overfit for the training data. ii) Batch size: * Determines how much data is processed with the weights while multiplying it. * Value usually will be 64, 128, 256. iii) Epoch: * It refers to one full training process of the training data. Its value usually will be 10, 50, 100.",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Hyperparameters Overview\",\"answer_text\":\"It controls how the network learns from the training data. Some of the hyperparameters are listed below.\"},{\"question_number\":\"i\",\"question_text\":\"Learning Rate (\\alpha)\",\"answer_text\":\"* Determines how big a step a model can usually take while learning.\\n* Its value usually be $\\mathbf{0.1, 0.001, 0.01}$.\\n* If learning rate is **high**, the model does not learn properly.\\n* If learning rate is **low**, the model will **overfit** for the training data.\"},{\"question_number\":\"ii\",\"question_text\":\"Batch size\",\"answer_text\":\"* Determines how much data is processed with the weights while multiplying it.\\n* Value usually will be $\\mathbf{64, 128, 256}$.\"},{\"question_number\":\"iii\",\"question_text\":\"Epoch\",\"answer_text\":\"* It refers to **one full training process** of the training data.\\n* Its value usually will be $\\mathbf{10, 50, 100}$.\"}]"
    },
    {
      "image": "IMG-20251026-WA0308.jpg",
      "ocr_text": "backpropagation where the gradients will decrease and we cannot calculate the optimal weights. * In CNN, Padding helps reduce the vanishing gradient problem. 3. Non-Linear Activation function: * Non-linear Activation function introduces non-linearity into the neural network, which makes them to understand and learn complex patterns. e.g.: Sigmoid, Tanh, ReLu. 4. Pooling Layer: * Pooling Layer is used to reduce the spatial size of the feature Map. Types: * Helps reduce overfitting. * Max Pooling, Average Pooling. 5. Layer Types in CNN: * There are three important layers in CNN. i) Convolutional Layer ii) Pooling Layer iii) Dropout.",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Gradient Problem and Padding\",\"answer_text\":\"...in backpropagation where the gradients will decrease and we cannot calculate the optimal weights. * In **CNN**, **Padding** helps reduce the **vanishing gradient problem**.\"},{\"question_number\":3,\"question_text\":\"Non-Linear Activation function\",\"answer_text\":\"* **Non-linear Activation function** introduces **non-linearity** into the neural network, which makes them to understand and learn **complex patterns**.\\ne.g.: **Sigmoid, Tanh, ReLu**.\"},{\"question_number\":4,\"question_text\":\"Pooling Layer\",\"answer_text\":\"* **Pooling Layer** is used to **reduce the spatial size of the feature Map**.\\nTypes:\\n* Helps reduce **overfitting**.\\n* **Max Pooling, Average Pooling**.\"},{\"question_number\":5,\"question_text\":\"Layer Types in CNN\",\"answer_text\":\"* There are **three important layers** in CNN.\\ni) **Convolutional Layer**\\nii) **Pooling Layer**\\niii) **Dropout**.\"}]"
    },
    {
      "image": "IMG-20251026-WA0301.jpg",
      "ocr_text": "* It can recognize patterns even when it is in top right or bottom of the image. * For example, in a cat image, even if the cat's ear is in top right or bottom, the CNN can easily recognize the cat's ear. Disadvantages: * It has high computational cost with deep networks. Part B (5 marks) 7. Loss function for classification and Regression. Loss function for classification: * Mean Squared Error (MSE) is calculated by taking the difference between the actual value and prediction value of the model and squaring it, and averaging them.",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"CNN Advantages (Pattern Recognition)\",\"answer_text\":\"* It can **recognize patterns** even when it is in top right or bottom of the image.\\n* **For example**, in a cat image, even if the cat's ear is in top right or bottom, the CNN can easily recognize the cat's ear.\"},{\"question_number\":\"undefined\",\"question_text\":\"CNN Disadvantages\",\"answer_text\":\"* It has **high computational cost** with **deep networks**.\"},{\"question_number\":7,\"question_text\":\"Loss function for classification and Regression (MSE)\",\"answer_text\":\"**Loss function for classification**:\\n* **Mean Squared Error (MSE)** is calculated by taking the **difference between the actual value and prediction value** of the model and **squaring it, and averaging them**.\"}]"
    },
    {
      "image": "IMG-20251026-WA0306.jpg",
      "ocr_text": "ii) Dominant feature extraction: * It extracts the dominant features like corner, edge from the feature map. * It shows it helps extract only key features. iii) Translation invariance: * It is to recognize the pattern even if it is in top, right or bottom left of the image. eg: Feature Map [2x2 grid example showing numbers] Max Pooling of 2x2 stride [Max Pooling output example]. Role in feature extraction: * It reduces the size of the feature map.",
      "json_output": "[{\"question_number\":\"ii\",\"question_text\":\"Dominant feature extraction\",\"answer_text\":\"* It extracts the **dominant features** like **corner, edge** from the **feature map**.\\n* It shows it helps extract only **key features**.\"},{\"question_number\":\"iii\",\"question_text\":\"Translation invariance\",\"answer_text\":\"* It is to recognize the **pattern** even if it is in **top, right or bottom left** of the image.\\neg: Feature Map [2x2 grid example showing numbers] **Max Pooling** of 2x2 stride [Max Pooling output example].\"},{\"question_number\":\"undefined\",\"question_text\":\"Role in feature extraction (Pooling)\",\"answer_text\":\"* It **reduces the size of the feature map**.\"}]"
    },
    {
      "image": "IMG-20251026-WA0311.jpg",
      "ocr_text": "Types: -> Sigmoid -> Tanh -> ReLu -> SoftMax. ii) Pooling Layer: Pooling Layer is defined as to reduce the spatial dimensions of length and width to retain the important information from the features for the deep neural network. (called as pooling layer). Types of pooling: -> Max pooling (dominant) -> Average pooling (mean) -> Global pooling. 5) Building Block of Convolutional Neural Network: In Convolutional neural network consists of several layers which vary on different model. The common CNN layers are: -> Input -> Convolution layer",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Function Types\",\"answer_text\":\"Types:\\n-> Sigmoid\\n-> Tanh\\n-> ReLu\\n-> SoftMax\"},{\"question_number\":\"ii\",\"question_text\":\"Pooling Layer\",\"answer_text\":\"**Pooling Layer** is defined as to **reduce the spatial dimensions of length and width** to retain the important information from the features for the deep neural network (called as **pooling layer**).\\nTypes of pooling:\\n-> **Max pooling** (dominant)\\n-> **Average pooling** (mean)\\n-> **Global pooling**.\"},{\"question_number\":5,\"question_text\":\"Building Block of Convolutional Neural Network\",\"answer_text\":\"In **Convolutional neural network** consists of several layers which vary on different models. The common CNN layers are:\\n-> Input\\n-> Convolution layer...\"}]"
    },
    {
      "image": "IMG-20251026-WA0305.jpg",
      "ocr_text": "iv) Momentum: * It is used to adjust the weights in the network. * Helps to generalize well. v) Sparsity: * It is used to make some dead neurons deactivate. * Helps reduce overfitting. 9. Pooling Layer: * Pooling layer in CNN is used to reduce the spatial size of the feature map. * It has few functions and they are listed below: i) Reduce Dimensionality: * It reduces the dimensionality of the feature map. * Thus reducing the computational cost to process all the values.",
      "json_output": "[{\"question_number\":\"iv\",\"question_text\":\"Momentum\",\"answer_text\":\"* It is used to **adjust the weights** in the network.\\n* Helps to **generalize well**.\"},{\"question_number\":\"v\",\"question_text\":\"Sparsity\",\"answer_text\":\"* It is used to make some **dead neurons deactivate**.\\n* Helps reduce **overfitting**.\"},{\"question_number\":9,\"question_text\":\"Pooling Layer\",\"answer_text\":\"* **Pooling layer in CNN** is used to **reduce the spatial size of the feature map**.\\n* It has few functions and they are listed below:\\ni) **Reduce Dimensionality**:\\n* It **reduces the dimensionality** of the feature map.\\n* Thus reducing the computational cost to process all the values.\"}]"
    },
    {
      "image": "IMG-20251026-WA0314.jpg",
      "ocr_text": "* And it converts the prediction based on the grid wise method. Convolution based on neural network for the image and video based model. * Which converts it into a grid and makes the prediction. 1. Pooling Layer: Pooling layer which is based on to reduce the Spatial dimensions [length and width] to retain the important information from the features in Deep kernel network. * It can be separated by the values based on following features and the ways of pooling: max pooling, average pooling and global pooling. everything is only based on the layer. Convolutional Layer: * Convolutional layer which extract the features by the weighted sum method and it will be converted into its separated form from the model. Prediction make Convolutional...",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Convolutional Network and Prediction\",\"answer_text\":\"* It converts the prediction based on the **grid wise method**.\\n* Convolution based on neural network for the **image and video based model**.\\n* Which converts it into a **grid** and makes the **prediction**.\"},{\"question_number\":1,\"question_text\":\"Pooling Layer\",\"answer_text\":\"**Pooling layer** is based on to **reduce the Spatial dimensions [length and width]** to retain the **important information** from the features in **Deep kernel network**.\\n* It can be separated by the values based on following features and the ways of pooling: **max pooling, average pooling and global pooling**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Convolutional Layer\",\"answer_text\":\"* **Convolutional layer** which extracts the features by the **weighted sum method** and it will be converted into its separated form from the model. Prediction make Convolutional...\"}]"
    },
    {
      "image": "IMG-20251026-WA0302.jpg",
      "ocr_text": "$MSE = 1 / 2M \\sum (\\hat{y}_i - y_i)^2$. $\\hat{y}_i$: Predicted value. $y_i$: Actual value. Adv: * It is differentiable. Disadv: * Sensitive to outliers. ii) Mean Absolute Error (MAE): * MAE is calculated by taking absolute difference in actual value and predicted value and averaging them. $MAE = 1 / N \\sum_{i=1}^{N} |\\hat{y}_i - y_i|$. Adv: * It is not sensitive to outliers. Disadv: * Cannot apply gradient descent directly. iii) Huber Loss: * Huber Loss is calculated by combining MAE and MSE. * $Huber Loss = ...$ Adv: * More robust to outliers. Disadv: * Usage of additional parameter $\\delta$.",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Mean Squared Error (MSE)\",\"answer_text\":\"$$MSE = \\frac{1}{2M} \\sum (\\hat{y}_i - y_i)^2$$\\n* $\\hat{y}_i$: Predicted value. $y_i$: Actual value.\\n**Adv**: * It is **differentiable**.\\n**Disadv**: * **Sensitive to outliers**.\"},{\"question_number\":\"ii\",\"question_text\":\"Mean Absolute Error (MAE)\",\"answer_text\":\"* **MAE** is calculated by taking **absolute difference** in actual value and predicted value and **averaging them**.\\n$$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{y}_i - y_i|$$\\n**Adv**: * It is **not sensitive to outliers**.\\n**Disadv**: * Cannot apply **gradient descent directly**.\"},{\"question_number\":\"iii\",\"question_text\":\"Huber Loss\",\"answer_text\":\"* **Huber Loss** is calculated by **combining MAE and MSE**.\\n**Adv**: * More **robust to outliers**.\\n**Disadv**: * Usage of **additional parameter $\\delta$**.\"}]"
    },
    {
      "image": "IMG-20251026-WA0313.jpg",
      "ocr_text": "* Input layer passes the weighted biases to the Dense layer. It extracts the required features to describe the process. After the Convolutional Layer should be added in between the Dense layer and Input layer. The Activation threshold value is not active when it reaches the threshold value. In Convolutional Neural Network it approaches the Dense Layer. The weight passes through the pooling layer and it reduces the Spatial size. It retains the important information from the features. [Additional notes on Convolutional Neural Network]",
      "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Layer Interaction and Data Flow\",\"answer_text\":\"* **Input layer** passes the **weighted biases** to the **Dense layer**. It extracts the required features to describe the process.\\n* The **Convolutional Layer** should be added in between the **Dense layer** and **Input layer**.\\n* The **Activation threshold value** is not active when it reaches the threshold value.\"},{\"question_number\":\"undefined\",\"question_text\":\"CNN Data Flow (Cont.)\",\"answer_text\":\"In **Convolutional Neural Network** it approaches the **Dense Layer**. The weight passes through the **pooling layer** and it **reduces the Spatial size**. It retains the **important information** from the features.\"}]"
    }{
        "image": "IMG-20251026-WA0317.jpg",
        "ocr_text": "Part-C 10) Neural networks: * Neural networks is the back bone of deep learning. * The neural networks architecture is inspired by human brain and it learn pattern in the way how human brain learn. * Today, modern AI mainly utilizing the deep learning algorithm behind the search. * The main difference of neural network from traditional ML is it can understand complex patterns like text, image, audio and multiple things at same time. * The fundamental components of neural network is input layer, hidden layer and output layer. Input layer: * This is the first layer of the network used to get input from the network.",
        "json_output": "[{\"question_number\":10,\"question_text\":\"Neural Networks\",\"answer_text\":\"* **Neural networks** is the **back bone of deep learning**.\\n* The neural networks architecture is **inspired by human brain** and learns patterns the way the human brain does.\\n* Today, modern AI mainly utilizes the deep learning algorithm.\\n* The main difference from traditional ML is it can understand **complex patterns** like text, image, audio and multiple things at the same time.\\n* The fundamental components are **input layer, hidden layer and output layer**.\\n\\n**Input layer**:\\n* This is the first layer of the network used to get input for the network.\"}]"
      },
      {
        "image": "IMG-20251026-WA0318.jpg",
        "ocr_text": "Output layers: * It is the last layer of the network which can give output of the network. * For classification task, the size of output layer equals the number of classes. Hidden layers: * Hidden layer is the main thing in neural network which decides how complex the network is to be. * The number of hidden layer is determined by complexity of the problem to be solved. Perceptron is a simple neural network which can handle simple logical tasks using fuzzy logic.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Output Layer\",\"answer_text\":\"* It is the **last layer** of the network which gives the output.\\n* For classification task, the size of the output layer equals the **number of classes**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Hidden Layers\",\"answer_text\":\"* **Hidden layer** is the main thing in the neural network, deciding how **complex** the network is to be.\\n* The number of hidden layers is determined by the **complexity of the problem** to be solved.\"},{\"question_number\":\"undefined\",\"question_text\":\"Perceptron\",\"answer_text\":\"* As described before in neural networks, a **perceptron** is a **simple neural network** which can handle simple logical tasks using fuzzy logic.\"}]"
      },
      {
        "image": "IMG-20251026-WA0315.jpg",
        "ocr_text": "* The layer of the Building blocks of convolution neural network should handle the performance based prediction. It applied the mathematical solutions to figure out the problem. The following properties are the Building blocks of convolution neural network: Input layer, Convolutional layer, Pooling layer, Dense layer, Output layer. 17) Activation function: * Defined as a mathematical function that is applied and improves the quality of models. The activation function activates when the threshold is given by above.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Building Blocks of CNN (Performance & Layers)\",\"answer_text\":\"* The layer of the **Building blocks** of convolution neural network should handle the **performance based prediction**. It applied the **mathematical solutions** to figure out the problem.\\n* The following layers are the **Building blocks of convolution neural network**:\\nInput layer, Convolutional layer, Pooling layer, Dense layer, Output layer.\"},{\"question_number\":17,\"question_text\":\"Activation function\",\"answer_text\":\"* Defined as a **mathematical function** that is applied and **improves the quality of models**.\\n* The activation function **activates when the threshold is given** by above.\"}]"
      },
      {
        "image": "IMG-20251026-WA0314.jpg",
        "ocr_text": "* And it converts the prediction based on the grid wise method. Convolution based on neural network for the image and video based model. * Which converts it into a grid and makes the prediction. 1. Pooling Layer: Pooling layer which is based on to reduce the Spatial dimensions [length and width] to retain the important information from the features in Deep kernel network. * It can be separated by the values based on following features and the ways of pooling: max pooling, average pooling and global pooling. everything is only based on the layer. Convolutional Layer: * Convolutional layer which extract the features by the weighted sum method and it will be converted into its separated form from the model. Prediction make Convolutional...",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Convolutional Network and Prediction\",\"answer_text\":\"* And it converts the prediction based on the **grid wise method**.\\n* Convolution based on neural network is suitable for the **image and video based model**.\\n* Which converts it into a **grid** and makes the **prediction**.\"},{\"question_number\":1,\"question_text\":\"Pooling Layer\",\"answer_text\":\"**Pooling layer** is based on to **reduce the Spatial dimensions [length and width]** to retain the **important information** from the features in **Deep kernel network**.\\n* It can be separated by the values based on following features and the ways of pooling: **max pooling, average pooling and global pooling**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Convolutional Layer\",\"answer_text\":\"* **Convolutional layer** which extracts the features by the **weighted sum method** and it will be converted into its separated form from the model. Prediction make Convolutional...\"}]"
      },
      {
        "image": "IMG-20251026-WA0311.jpg",
        "ocr_text": "Types: -> Sigmoid -> Tanh -> ReLu -> SoftMax. ii) Pooling Layer: Pooling Layer is defined as to reduce the spatial dimensions of length and width to retain the important information from the features for the deep neural network. (called as pooling layer). Types of pooling: -> Max pooling (dominant) -> Average pooling (mean) -> Global pooling. 5) Building Block of Convolutional Neural Network: In Convolutional neural network consists of several layers which vary on different model. The common CNN layers are: -> Input -> Convolution layer",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Function Types\",\"answer_text\":\"Types:\\n-> Sigmoid\\n-> Tanh\\n-> ReLu\\n-> SoftMax\"},{\"question_number\":\"ii\",\"question_text\":\"Pooling Layer\",\"answer_text\":\"**Pooling Layer** is defined as to **reduce the spatial dimensions of length and width** to retain the important information from the features for the deep neural network (called as **pooling layer**).\\nTypes of pooling:\\n-> **Max pooling** (dominant)\\n-> **Average pooling** (mean)\\n-> **Global pooling**.\"},{\"question_number\":5,\"question_text\":\"Building Block of Convolutional Neural Network\",\"answer_text\":\"In **Convolutional neural network** consists of several layers which vary on different models. The common CNN layers are:\\n-> Input\\n-> Convolution layer...\"}]"
      },
      {
        "image": "IMG-20251026-WA0316.jpg",
        "ocr_text": "* $f(x) = (e^x - e^{-x}) / (e^x + e^{-x})$. [Continued from previous page] iii) Softmax: * Softmax activation is defined as getting the maximum values of 0 to 1 and it can be calculated using given values. * $f(x) = \\max(0, 1 - y \\cdot f(x))$. iv) ReLu: * Regression Logic Unit which is defined as the ReLu which is a more efficient activation function. * $f(x) = e^x / \\sum e^x$. Part-B. 9) Pooling layer: * Pooling layer in the convolutional neural network is defined as to reduce the spatial dimension [length and width] to retain the important information from the features. * It reduces the length and width from the filter separated channel based on the formula: $ (n_h - f + 1) / s \\times (n_w - f + 1) / s \\times n_c $. Types: -> Max pooling -> Global pooling -> Average pooling. Max pooling: To filter the dimension based on the maximum values to calculate and it will be added to the channels. Average pooling: Average pooling is defined as getting the average of all values in the different features.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Functions (Softmax, ReLu)\",\"answer_text\":\"* $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ (Tanh continuation).\\n\\niii) **Softmax**:\\n* **Softmax activation** is defined as getting the maximum values of $\\mathbf{0}$ to $\\mathbf{1}$.\\n* $f(x) = \\max(0, 1 - y \\cdot f(x))$ (Note: This formula looks like Hinge Loss, not standard Softmax).\\n\\niv) **ReLu**:\\n* **Regression Logic Unit** defined as **ReLu** is a more efficient activation function.\\n* $f(x) = \\frac{e^x}{\\sum e^x}$ (Note: This formula is for Softmax, not standard ReLu).\\n\\n**Part-B**.\"},{\"question_number\":9,\"question_text\":\"Pooling layer\",\"answer_text\":\"* **Pooling layer** in CNN is defined as to **reduce the spatial dimension [length and width]** to retain the important information from the features.\\n* It reduces the length and width based on the formula: $ \\frac{(n_h - f + 1)}{s} \\times \\frac{(n_w - f + 1)}{s} \\times n_c $ (Output dimensions).\\nTypes:\\n-> **Max pooling**\\n-> **Global pooling**\\n-> **Average pooling**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Max and Average Pooling Definition\",\"answer_text\":\"**Max pooling**:\\nTo filter the dimension based on the **maximum values** and add it to the channels.\\n\\n**Average pooling**:\\nDefined as getting the **average of all values** in the different features.\"}]"
      },
      {
        "image": "IMG-20251026-WA0313.jpg",
        "ocr_text": "* Input layer passes the weighted biases to the Dense layer. It extracts the required features to describe the process. After the Convolutional Layer should be added in between the Dense layer and Input layer. The Activation threshold value is not active when it reaches the threshold value. In Convolutional Neural Network it approaches the Dense Layer. The weight passes through the pooling layer and it reduces the Spatial size. It retains the important information from the features. [Additional notes on Convolutional Neural Network]",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Layer Interaction and Data Flow\",\"answer_text\":\"* **Input layer** passes the **weighted biases** to the **Dense layer**. It extracts the required features to describe the process.\\n* The **Convolutional Layer** should be added in between the **Dense layer** and **Input layer**.\\n* The **Activation threshold value** is not active when it reaches the threshold value.\\n\\nIn **Convolutional Neural Network** it approaches the **Dense Layer**. The weight passes through the **pooling layer** and it **reduces the Spatial size**. It retains the **important information** from the features.\"}]"
      },
      {
        "image": "IMG-20251026-WA0308.jpg",
        "ocr_text": "backpropagation where the gradients will decrease and we cannot calculate the optimal weights. * In CNN, Padding helps reduce the vanishing gradient problem. 3. Non-Linear Activation function: * Non-linear Activation function introduces non-linearity into the neural network, which makes them to understand and learn complex patterns. e.g.: Sigmoid, Tanh, ReLu. 4. Pooling Layer: * Pooling Layer is used to reduce the spatial size of the feature Map. Types: * Helps reduce overfitting. * Max Pooling, Average Pooling. 5. Layer Types in CNN: * There are three important layers in CNN. i) Convolutional Layer ii) Pooling Layer iii) Dropout.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Gradient Problem and Padding\",\"answer_text\":\"...in backpropagation where the gradients will decrease and we cannot calculate the optimal weights. * In **CNN**, **Padding** helps reduce the **vanishing gradient problem**.\"},{\"question_number\":3,\"question_text\":\"Non-Linear Activation function\",\"answer_text\":\"* **Non-linear Activation function** introduces **non-linearity** into the neural network, which makes them to understand and learn **complex patterns**.\\ne.g.: **Sigmoid, Tanh, ReLu**.\"},{\"question_number\":4,\"question_text\":\"Pooling Layer\",\"answer_text\":\"* **Pooling Layer** is used to **reduce the spatial size of the feature Map**.\\nTypes:\\n* Helps reduce **overfitting**.\\n* **Max Pooling, Average Pooling**.\"},{\"question_number\":5,\"question_text\":\"Layer Types in CNN\",\"answer_text\":\"* There are **three important layers** in CNN.\\ni) **Convolutional Layer**\\nii) **Pooling Layer**\\niii) **Dropout**.\"}]"
      },
      {
        "image": "IMG-20251026-WA0319.jpg",
        "ocr_text": "Multi layer perceptron: (MLP) * Multi-layer perceptron are used mainly in deep Neural networks like transformers to bypass content dependency. * Not like single perceptron MLP has multiple Hidden layers. Forward propagation: * Forward propagation happens for each batch updates of data in which randomly initialized on an existing weights. * The weight initialization can be done by methods like: Xavier initialization, Kaiming initialization. Back ward propagation: * After forward propagation there will be a randomly predicted value which may differ from actual output.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Multi-layer Perceptron (MLP)\",\"answer_text\":\"* **Multi-layer perceptron** are used mainly in deep Neural networks like transformers to bypass content dependency.\\n* Unlike single perceptron, MLP has **multiple Hidden layers**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Forward propagation\",\"answer_text\":\"* **Forward propagation** happens for each batch updates of data in which randomly initialized on an existing weights.\\n* The **weight initialization** can be done by methods like:\\n-> Xavier initialization\\n-> Kaiming initialization.\"},{\"question_number\":\"undefined\",\"question_text\":\"Backward propagation\",\"answer_text\":\"* After forward propagation, there will be a randomly predicted value which may differ from the actual output.\"}]"
      },
      {
        "image": "IMG-20251026-WA0320.jpg",
        "ocr_text": "The difference between actual and predicted value is calculated by loss function. Types: * Mean squared error: $MSE = 1 / R \\sum (Y - \\hat{Y})^2$. * Mean Absolute error: $MAE = 1 / N \\sum |Y - \\hat{Y}|$. * Cross entropy loss. Gradient descent: Loss value will be updated by in gradient which is multi-dimensional space. Back propagation: It makes gradient push to convergence by chain rule which update weights by considering all other nodes. $dL/dx = dL/dy \\cdot dy/dx$.",
        "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Loss Function and Types\",\"answer_text\":\"The difference between actual and predicted value is calculated by **loss function**.\\nTypes:\\n* **Mean squared error** (MSE): $$MSE = \\frac{1}{R} \\sum (Y - \\hat{Y})^2$$\\n* **Mean Absolute error** (MAE): $$MAE = \\frac{1}{N} \\sum |Y - \\hat{Y}|$$\\n* **Cross entropy loss**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Gradient Descent and Back Propagation\",\"answer_text\":\"**Gradient descent**:\\n* Loss value will be updated by in gradient which is multi-dimensional space.\\n\\n**Back propagation**:\\n* It makes gradient push to **convergence** by **chain rule**, which updates weights by considering all other nodes.\\n* $\\frac{dL}{dx} = \\frac{dL}{dy} \\cdot \\frac{dy}{dx}$.\"}]"
      }
 {
    "image": "IMG-20251026-WA0326.jpg",
    "ocr_text": "Loss function for regression: Mean squared error: MSE is calculated by average of square of actual - predicted $MSE = 1 / R \\sum (Y - \\hat{Y})^2$. Mean absolute error: MAE is calculated by absolute average of actual - predicted value $MAE = 1 / N \\sum |Y - \\hat{Y}|$. Huber loss: Huber loss is a combination of MSE and MAE used in complex regression task. Loss function for classification: Cross entropy loss: Cross entropy loss compare logit value of end outputs of each class and provide loss.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Loss function for regression\",\"answer_text\":\"**Mean squared error (MSE)**:\\nMSE is calculated by **average of square of actual - predicted**.\\n$$MSE = \\frac{1}{R} \\sum (Y - \\hat{Y})^2$$\\n**Mean absolute error (MAE)**:\\nMAE is calculated by **absolute average of actual - predicted value**.\\n$$MAE = \\frac{1}{N} \\sum |Y - \\hat{Y}|$$\\n**Huber loss**:\\nHuber loss is a **combination of MSE and MAE** used in complex regression tasks.\"},{\"question_number\":\"undefined\",\"question_text\":\"Loss function for classification (Cross Entropy)\",\"answer_text\":\"**Cross entropy loss**:\\nCross entropy loss compares the **logit value of end outputs** of each class and provides loss.\"}]"
  },
  {
    "image": "IMG-20251026-WA0330.jpg",
    "ocr_text": "2) Vanishing gradient: Vanishing gradient is the main problem in NN where the gradient become too low and it will never meet the convergence. 3) Activation function: introduce non-linearity in network and decide the trigger the neuron. Types: * Sigmoid * Softmax * Tanh * ReLu. 4) Pooling layers: Pooling layers are used to reduce the spatial length of the image & to make CNN computationally efficient.",
    "json_output": "[{\"question_number\":2,\"question_text\":\"Vanishing gradient\",\"answer_text\":\"**Vanishing gradient** is the main problem in NN where the **gradient become too low** and it will **never meet the convergence**.\"},{\"question_number\":3,\"question_text\":\"Activation function\",\"answer_text\":\"**Activation function** introduces **non-linearity** in the network and decides the **trigger** the neuron.\\nTypes:\\n* Sigmoid\\n* Softmax\\n* Tanh\\n* ReLu\"},{\"question_number\":4,\"question_text\":\"Pooling layers\",\"answer_text\":\"**Pooling layers** are used to **reduce the spatial length of the image** and to make **CNN computationally efficient**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0322.jpg",
    "ocr_text": "Stride: Stride size shows how many steps should the kernel move from each step. Pooling layers: * Pooling layers are used to reduce the spatial size of the image to make the network computationally efficient. Types: * Max pooling: It takes the specified size of image and picks only the max value from it (max(xi)). * Average pooling: * In most of the complex problems, average pooling will be used. * It takes the specified size of image and picks the average value of the matrix. This process continues for entire image.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Stride and Pooling Layers\",\"answer_text\":\"**Stride** size shows how many **steps** should the **kernel move** from each step.\\n\\n**Pooling layers**:\\n* **Pooling layers** are used to **reduce the spatial size of the image** to make the network **computationally efficient**.\\nTypes:\\n* **Max pooling**: It takes the specified size of image and picks only the **max value** from it ($\\max(x_i)$).\\n* **Average pooling**: In most of the **complex problems**, average pooling will be used. It takes the specified size of image and picks the **average value of the matrix**. This process continues for the entire image.\"}]"
  },
  {
    "image": "IMG-20251026-WA0324.jpg",
    "ocr_text": "Part-B. 6) Neural networks: * Neural networks are used to build deep learning model to generalize them on various patterns like voice, text and image. * Brain with NN: The neural networks architecture is inspired by human brain and it learn pattern in the way how human brain learns. * In human brain the neurons are interconnected and a chemical reaction and biological movement occurs to learn. Likewise, neural network neurons are interconnected to learn. The fundamental components of neural network is input layer, hidden layer and output layer.",
    "json_output": "[{\"question_number\":6,\"question_text\":\"Neural networks\",\"answer_text\":\"* **Neural networks** are used to build a **deep learning model** to **generalize** them on various patterns like **voice, text and image**.\\n\\n**Brain with NN**:\\n* The neural networks architecture is **inspired by the human brain** and it learns patterns the way the human brain learns.\\n* In the human brain, neurons are interconnected, and a chemical reaction and biological movement occurs to learn. Likewise, neural network neurons are interconnected to learn.\\n* The **fundamental components** of neural network are **input layer, hidden layer and output layer**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0327.jpg",
    "ocr_text": "Binary Cross Entropy (BCE): It is used of binary classification task use sigmoid output. 8) Hyper Parameters: Neural network require some hyper parameters to customize the training for our needs. Parameters includes: Learning rate: Learning rate determines how much a network learns for each epoch. It is usually 0.1 and changed while training by optimizer. Epoch: Epoch says how many time should a network train on complete training data set. Batch: For all epochs we cannot load entire data in single attempt so we should specify size for data partition.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Binary Cross Entropy (BCE)\",\"answer_text\":\"**Binary Cross Entropy (BCE)** is used for **binary classification tasks** using **sigmoid output**.\"},{\"question_number\":8,\"question_text\":\"Hyper Parameters\",\"answer_text\":\"Neural networks require some **hyper parameters** to customize the training for our needs.\\n\\n**Learning rate**:\\n* **Learning rate** determines how much a network learns for each epoch.\\n* It is usually $\\mathbf{0.1}$ and is changed while training by the optimizer.\\n\\n**Epoch**:\\n* **Epoch** says how many times a network should train on the **complete training data set**.\\n\\n**Batch**:\\n* For all epochs we cannot load the entire data in a single attempt, so we should specify the size for **data partition**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0328.jpg",
    "ocr_text": "usually Batch size will be 32. Decreasing it may underfit the model. Weight initializations: In complex neural networks, weight initialization will be hard so it too will be passed as hyper parameter to the network. $\\rightarrow$ Kmin weight initialization $\\rightarrow$ Xavier weight initialization. 9) Pooling layer: Pooling layers are used to reduce the spatial size of the image, make the network computationally efficient. Types: * Max pooling: It takes the specified size of image and picks only the max value from it $(\\max(x_i))$",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Batch Size and Weight Initializations\",\"answer_text\":\"* Usually **Batch size** will be **32**.\\n* Decreasing it may **underfit** the model.\\n\\n**Weight initializations**:\\n* In complex neural networks, weight initialization is hard, so it is passed as a **hyper parameter** to the network.\\n$\\rightarrow$ **Kmin weight initialization**\\n$\\rightarrow$ **Xavier weight initialization**.\"},{\"question_number\":9,\"question_text\":\"Pooling layer\",\"answer_text\":\"**Pooling layers** are used to **reduce the spatial size of the image**, making the network **computationally efficient**.\\nTypes:\\n* **Max pooling**: It takes the specified size of image and picks only the **max value** from it ($\\max(x_i)$).\"}]"
  },
  {
    "image": "IMG-20251026-WA0323.jpg",
    "ocr_text": "Batch normalization: The image value may be highly intense and hard to compute inside the network. So, the best practice is applying Batch normalization for each layer. Flatten layer: The output from CNN may be 2 to 4 dimension. To convert it into 1 dimension, Flatten layer is included. Fully connected layer: * Fully connected layers take values of flatten layer and act as neural network. * It gives the final layer of the network and provides end output.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Batch Normalization\",\"answer_text\":\"* The image value may be **highly intense** and **hard to compute** inside the network.\\n* So, the best practice is applying **Batch normalization for each layer**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Flatten Layer\",\"answer_text\":\"* The output from CNN may be **2 to 4 dimension**. To convert it into **1 dimension**, **Flatten layer** is included.\"},{\"question_number\":\"undefined\",\"question_text\":\"Fully connected layer\",\"answer_text\":\"* **Fully connected layers** take values from the **Flatten layer** and act as a **neural network**.\\n* It gives the **final layer** of the network and provides the **end output**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0321.jpg",
    "ocr_text": "12) Convolutional neural network: * In early stage of Deep learning, handling images by ANN made the model more complex. * So, scientists invented Convolutional neural network to handle image by filtering the feature from image efficiently. Components of CNN: Convolutional layers: * Convolutional layer apply filter in parts of image which can extract horizontal and vertical lines from the image. * Kernel: Kernel determines the size of the filter and that same size of image matrix will be handled by filter and form a new image by extracted features.",
    "json_output": "[{\"question_number\":12,\"question_text\":\"Convolutional neural network\",\"answer_text\":\"* In early stage of Deep learning, handling images by ANN made the model more **complex**.\\n* So, scientists invented **Convolutional neural network** to handle the image by **filtering the feature from the image efficiently**.\\n\\n**Components of CNN**:\\n**Convolutional layers**:\\n* **Convolutional layer** applies a **filter** to parts of the image which can extract **horizontal and vertical lines** from the image.\\n* **Kernel**: The **Kernel** determines the **size of the filter**. That same size of image matrix will be handled by the filter and form a new image by **extracted features**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0329.jpg",
    "ocr_text": "Average pooling: * In most of the complex problems, average pooling will be used. * It takes the specified size of image and picks the average value of the matrix. This process continues for the entire image. Example: [Example showing Max Pooling of a matrix]. 1) Perceptron: * Perceptron is a simple neural network, which has input layer and output layer used for logical operation. * Multi layer perceptron has hidden layer with it and can understand complex pattern by Activation.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Average pooling\",\"answer_text\":\"* In most of the **complex problems**, **average pooling** will be used.\\n* It takes the specified size of image and picks the **average value of the matrix**. This process continues for the entire image.\\nExample: [Example showing Max Pooling of a matrix].\"},{\"question_number\":1,\"question_text\":\"Perceptron\",\"answer_text\":\"* **Perceptron** is a **simple neural network** which has **input layer and output layer** used for **logical operation**.\\n* **Multi layer perceptron** has a **hidden layer** with it and can understand **complex patterns** by **Activation**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0325.jpg",
    "ocr_text": "Components of neural networks: Input layer: This is the first layer of the network used to get input for the network. Output layer: * It is the last layer of the network which can give output of the network. * For classification task the size of output layer equals to the numbers of classes. Hidden layers: The number of hidden layers is determined by the complexity of the problem to be solved. 7) Loss function: * In initial stage of training, neural networks predict wrong value. * To calculate the difference between predicted and actual value, loss function is used (y - ŷ).",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Components of neural networks (Input Layer)\",\"answer_text\":\"**Input layer**:\\n* This is the **first layer** of the network used to get **input** for the network.\"},{\"question_number\":\"undefined\",\"question_text\":\"Components of neural networks (Output Layer)\",\"answer_text\":\"**Output layer**:\\n* It is the **last layer** of the network which can give **output**.\\n* For classification tasks, the size of the output layer equals the **number of classes**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Components of neural networks (Hidden Layers)\",\"answer_text\":\"**Hidden layers**:\\n* The **number of hidden layers** is determined by the **complexity of the problem** to be solved.\"},{\"question_number\":7,\"question_text\":\"Loss function\",\"answer_text\":\"* In the initial stage of training, neural networks predict the wrong value.\\n* To calculate the **difference between predicted and actual value**, **loss function** is used $(y - \\hat{y})$ (this represents the error term).\"}]"
  }
{
    "image": "IMG-20251026-WA0338.jpg",
    "ocr_text": "PART-C. 10. Neural network: * A computational model that mimics the human brain. * Composed of layers of neurons. * Input layer. * One or more Hidden layers. * Output layer. * Each neural processes data using weighted connections. i) Perceptron: * Introduced by Frank Rosenblatt (1958). * Used for binary classification. * Working: $y = f(\\sum_{i=1}^{n} w_i x_i + b)$",
    "json_output": "[{\"question_number\":10,\"question_text\":\"Neural network\",\"answer_text\":\"* A **computational model** that mimics the **human brain**.\\n* Composed of **layers of neurons**.\\n* **Input layer**.\\n* One or more **Hidden layers**.\\n* **Output layer**.\\n* Each neuron processes data using **weighted connections**.\"},{\"question_number\":\"i\",\"question_text\":\"Perceptron\",\"answer_text\":\"* Introduced by **Frank Rosenblatt (1958)**.\\n* Used for **binary classification**.\\n* Working: $$y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\"}]"
  },
  {
    "image": "IMG-20251026-WA0331.jpg",
    "ocr_text": "PART-A. 1. Perceptron: * Perceptron is the simplest type of artificial neural network consisting of a single layer of weights applied to inputs followed by an activation function. 2. Vanishing gradient: Vanishing gradient problem occurs when gradients become too small in backpropagation, causing earlier layers to learn slowly or not at all. 3. Non-linear Activation Function: * Non-linear activation function introduces non-linearity to the model enabling it to learn complex patterns. E.g.: ReLu, Sigmoid, and Tanh.",
    "json_output": "[{\"question_number\":1,\"question_text\":\"Perceptron\",\"answer_text\":\"* **Perceptron** is the **simplest type of artificial neural network** consisting of a single layer of weights applied to inputs followed by an **activation function**.\"},{\"question_number\":2,\"question_text\":\"Vanishing gradient\",\"answer_text\":\"**Vanishing gradient problem** occurs when **gradients become too small in backpropagation**, causing earlier layers to learn **slowly or not at all**.\"},{\"question_number\":3,\"question_text\":\"Non-linear Activation Function\",\"answer_text\":\"* **Non-linear activation function** introduces **non-linearity** to the model enabling it to learn **complex patterns**.\\nE.g.: **ReLu, Sigmoid, and Tanh**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0339.jpg",
    "ocr_text": "$w_i = weight$. $x_i = input$. $b = bias$. $f = activation function$. ii) Limitation of Perceptron: * Can only solve **linearly separable** problems. * Cannot solve **XOR problem**. iii) Multilayer Perceptron (MLP): * Solves **non-linear** problems. * Consists of **multiple layers**: Input $\\rightarrow$ Hidden $\\rightarrow$ Output.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Perceptron Components\",\"answer_text\":\"$w_i = \\text{weight}$. $x_i = \\text{input}$. $b = \\text{bias}$. $f = \\text{activation function}$.\"},{\"question_number\":\"ii\",\"question_text\":\"Limitations of Perceptron\",\"answer_text\":\"* Can only solve **linearly separable** problems.\\n* Cannot solve **XOR problem**.\"},{\"question_number\":\"iii\",\"question_text\":\"Multilayer Perceptron (MLP)\",\"answer_text\":\"* Solves **non-linear** problems.\\n* Consists of **multiple layers**:\\nInput $\\rightarrow$ Hidden $\\rightarrow$ Output.\"}]"
  },
  {
    "image": "IMG-20251026-WA0335.jpg",
    "ocr_text": "8. Hyper parameters: Set before training to control model behavior. i) Types: * Learning Rate: Step size in weight update. * Batch Size: No. of samples per gradient update. * Epochs: No. of times the dataset is passed through the model. * No of layers: Controls model depth and capacity. * Dropout rate: Prevent overfitting.",
    "json_output": "[{\"question_number\":8,\"question_text\":\"Hyper parameters\",\"answer_text\":\"**Hyper parameters** are set before training to **control model behavior**.\\ni) Types:\\n* **Learning Rate**: **Step size** in weight update.\\n* **Batch Size**: Number of samples per gradient update.\\n* **Epochs**: Number of times the dataset is passed through the model.\\n* **No of layers**: Controls model depth and capacity.\\n* **Dropout rate**: Prevents **overfitting**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0336.jpg",
    "ocr_text": "ii) Tuning Techniques include: * Grid Search. * Random Search. * Bayesian Optimization. 9. Function and working of Pooling layers: Pooling layers: * Reduce spatial dimension of feature maps. * Control overfitting. * Retain the essential feature maps. Types: * Max Pooling. * Average Pooling.",
    "json_output": "[{\"question_number\":\"ii\",\"question_text\":\"Tuning Techniques\",\"answer_text\":\"Tuning Techniques include:\\n* **Grid Search**.\\n* **Random Search**.\\n* **Bayesian Optimization**.\"},{\"question_number\":9,\"question_text\":\"Function and working of Pooling layers\",\"answer_text\":\"**Pooling layers**:\\n* **Reduce spatial dimension of feature maps**.\\n* **Control overfitting**.\\n* **Retain the essential feature maps**.\\nTypes:\\n* **Max Pooling**.\\n* **Average Pooling**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0332.jpg",
    "ocr_text": "4. Pooling layer: * A pooling layer reduces the spatial dimension of input feature and relays important information. Types: * Max pooling * Average pooling. 5. Type of Convolutional Neural Network: * Convolutional layer. * Activation layer. * Pooling layer. * Output layer. * Fully connected layer.",
    "json_output": "[{\"question_number\":4,\"question_text\":\"Pooling layer\",\"answer_text\":\"* A **pooling layer** reduces the **spatial dimension of the input feature** and relays **important information**.\\nTypes:\\n* **Max pooling**\\n* **Average pooling**.\"},{\"question_number\":5,\"question_text\":\"Type of Convolutional Neural Network Layers\",\"answer_text\":\"* **Convolutional layer**.\\n* **Activation layer**.\\n* **Pooling layer**.\\n* **Output layer**.\\n* **Fully connected layer**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0337.jpg",
    "ocr_text": "Max pooling: Takes max value from the window. Average pooling: Takes average value in the window. Role in Feature Extraction: * Reduces computation. * Provides translational invariance. * Highlights dominant features. * It is a key role of the feature extraction.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Max and Average Pooling\",\"answer_text\":\"**Max pooling**:\\nTakes **max value** from the window.\\n**Average pooling**:\\nTakes **average value** in the window.\"},{\"question_number\":\"undefined\",\"question_text\":\"Role in Feature Extraction (Pooling)\",\"answer_text\":\"Role in **Feature Extraction**:\\n* Reduces **computation**.\\n* Provides **translational invariance**.\\n* Highlights **dominant features**.\\n* It is a key role of the feature extraction.\"}]"
  },
  {
    "image": "IMG-20251026-WA0334.jpg",
    "ocr_text": "* Learning rule: Minimizes error by updating weights using gradient descent. Biological Neurons: * Connect and fire based on stimuli. * Synapses: are weights. * Dendrites: are input features/signals. * It is inspired by biological neurons.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Learning Rule\",\"answer_text\":\"* **Minimizes error** by **updating weights** using **gradient descent**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Biological Neurons (Inspiration for NN)\",\"answer_text\":\"* Biological neurons **connect and fire** based on **stimuli**.\\n* **Synapses** are the equivalent of **weights**.\\n* **Dendrites** are the equivalent of **input features/signals**.\\n* The NN is **inspired by biological neurons**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0333.jpg",
    "ocr_text": "PART-B. 6. Fundamental Principles: * Artificial Neuron: Inspired by biological neurons, receives input, applies weight, adds bias, passes through activation. * Layers: Input layer $\\rightarrow$ Hidden layer $\\rightarrow$ Output layer. * Weights & Biases: Determine neuron importance. * Activation Function: Introduces non-linearity.",
    "json_output": "[{\"question_number\":6,\"question_text\":\"Fundamental Principles (Artificial Neuron)\",\"answer_text\":\"* **Artificial Neuron**: Inspired by **biological neurons**, receives **input**, applies **weight**, adds **bias**, passes through **activation**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Layers, Weights & Activation Function\",\"answer_text\":\"* **Layers**: **Input layer** $\\rightarrow$ **Hidden layer** $\\rightarrow$ **Output layer**.\\n* **Weights & Biases**: Determine **neuron importance**.\\n* **Activation Function**: Introduces **non-linearity**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0340.jpg",
    "ocr_text": "* Common activation functions: ReLu, Sigmoid, Tanh. * Used in real applications: -> Digit recognition -> Sentiment analysis -> Stock prediction. 7. Back Propagation: * Learning algorithm to train neural networks. * Steps: a) Forward Pass: calculate output and record loss. b) Backward Pass: - use chain rule to compute gradient - calculate how loss changes with respect to each weight.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Functions and Applications\",\"answer_text\":\"* Common activation functions: **ReLu, Sigmoid, Tanh**.\\n* Used in real applications:\\n-> **Digit recognition**\\n-> **Sentiment analysis**\\n-> **Stock prediction**.\"},{\"question_number\":7,\"question_text\":\"Back Propagation\",\"answer_text\":\"* **Learning algorithm** to **train neural networks**.\\n* **Steps**:\\na) **Forward Pass**: calculate output and record loss.\\nb) **Backward Pass**:\\n- use **chain rule** to compute **gradient**\\n- calculate how loss changes with respect to each **weight**.\"}]"
  }
{
    "image": "IMG-20251026-WA0353.jpg",
    "ocr_text": "* An activation function in a neural network introduces non-linearity to the model. * It decides whether a neuron should be activated or not. It transforms the weighted sum of input. 4) Pooling layers: * A pooling layer reduces the dimension of input volume by applying operation to the image. * For example, max pooling and average pooling. * It helps to address overfitting. Types of CNN: * POSiT * Negati * NOT * AND * OR * XOR. It is a Convolutional Neural Network.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Activation Function\",\"answer_text\":\"* An **activation function** in a neural network introduces **non-linearity** to the model.\\n* It decides whether a neuron should be **activated or not**. It transforms the weighted sum of input.\"},{\"question_number\":4,\"question_text\":\"Pooling layers\",\"answer_text\":\"* A **pooling layer** reduces the **dimension of the input volume** by applying operation to the image.\\n* For example, **max pooling** and **average pooling**.\\n* It helps to address **overfitting**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Types of CNN (Logical Operations)\",\"answer_text\":\"Types of CNN [referring to logical gates/operations]:\\n* POSiT, Negati, NOT, AND, OR, XOR. It is a Convolutional Neural Network.\"}]"
  },
  {
    "image": "IMG-20251026-WA0361.jpg",
    "ocr_text": "9. Convolutional Neural Network: * Convolutional Neural Network is one of the most important category in image classification and recognition. * Neural Network used for: Scene labeling, object detection, face recognition. * Specialized in processing data like for Pixels such as a image. * A digital image is a binary representation and it contains a series of Pixels arranged in a grid. Pooling Layers: * Pooling layers is used in convolutional neural network for image processing and recognition. * It is used to reduce the spatial dimensions on the image filters.",
    "json_output": "[{\"question_number\":9,\"question_text\":\"Convolutional Neural Network (CNN)\",\"answer_text\":\"* **Convolutional Neural Network** is one of the most important category in **image classification and recognition**.\\n* Neural Network used for: **Scene labeling, object detection, face recognition**.\\n* Specialized in processing data like **Pixels** in an image.\\n* A digital image is a binary representation and contains a series of Pixels arranged in a **grid**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Pooling Layers (CNN Role)\",\"answer_text\":\"* **Pooling layers** is used in convolutional neural network for **image processing and recognition**.\\n* It is used to **reduce the spatial dimensions** on the image filters.\"}]"
  },
  {
    "image": "IMG-20251026-WA0342.jpg",
    "ocr_text": "ii) Convolutional Layer: * Applies filters (kernel) to extract features. * Used stride and padding to control output size. * Produces feature maps. iii) Activation Function (ReLu): * Applies non-linearity after convolution. * ReLu = max (0, x). * Allows deep learning by introducing complexity. iv) Pooling layers: * Reduces feature map size. * Max Pooling picks max value in region.",
    "json_output": "[{\"question_number\":\"ii\",\"question_text\":\"Convolutional Layer\",\"answer_text\":\"* Applies **filters (kernel)** to **extract features**.\\n* Uses **stride and padding** to **control output size**.\\n* Produces **feature maps**.\"},{\"question_number\":\"iii\",\"question_text\":\"Activation Function (ReLu)\",\"answer_text\":\"* Applies **non-linearity** after convolution.\\n* **ReLu** = $\\max(0, x)$.\\n* Allows **deep learning** by introducing **complexity**.\"},{\"question_number\":\"iv\",\"question_text\":\"Pooling layers\",\"answer_text\":\"* **Reduces feature map size**.\\n* **Max Pooling** picks the **max value** in the region.\"}]"
  },
  {
    "image": "IMG-20251026-WA0349.jpg",
    "ocr_text": "Common Loss function: Mean squared error (MSE): $MSE = 1 / N \\sum (y_i - \\hat{y}_i)^2$. * More sensitive to outliers. Mean absolute error (MAE): $MAE = 1 / N \\sum |y_i - \\hat{y}_i|$. Categorical cross entropy is used for multi-class classification in loss function. This is the formula of $Loss = -\\sum (y \\log \\hat{y} + (1-y) \\log (1-\\hat{y}))$. 12. Loss function for classification: $Loss = -\\sum y_i \\log \\hat{y}_i$. Line provides loss function for classification classes. * It is mainly used when classes are more than two.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Common Loss function (Regression)\",\"answer_text\":\"**Mean squared error (MSE)**:\\n$$MSE = \\frac{1}{N} \\sum (y_i - \\hat{y}_i)^2$$\\n* More sensitive to **outliers**.\\n**Mean absolute error (MAE)**:\\n$$MAE = \\frac{1}{N} \\sum |y_i - \\hat{y}_i|$$\\n* **Categorical cross entropy** is used for **multi-class classification**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Binary Cross Entropy Formula\",\"answer_text\":\"This is the formula of **Binary Cross Entropy**:\\n$$Loss = -\\sum \\left(y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})\\right)$$\"},{\"question_number\":12,\"question_text\":\"Loss function for classification (Categorical Cross Entropy)\",\"answer_text\":\"$$Loss = -\\sum y_i \\log \\hat{y}_i$$\\n* It is mainly used when classes are **more than two** (multi-class classification).\"}]"
  },
  {
    "image": "IMG-20251026-WA0343.jpg",
    "ocr_text": "* Avg Pooling: Takes average values. * Reduces computation and overfitting. v) Dropout layer: * Randomly drops neurons during training. * Prevents overfitting. vi) Flatten layer: * Converts 2D feature map to 1D vectors for classification. * It is a type of layer. vii) Fully connected layer: * Final decision making layer. * Each neuron connects to all output from previous layers.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Average Pooling\",\"answer_text\":\"* **Avg Pooling** takes **average values**.\\n* **Reduces computation** and **overfitting**.\"},{\"question_number\":\"v\",\"question_text\":\"Dropout layer\",\"answer_text\":\"* **Randomly drops neurons** during training.\\n* **Prevents overfitting**.\"},{\"question_number\":\"vi\",\"question_text\":\"Flatten layer\",\"answer_text\":\"* **Converts 2D feature map to 1D vectors** for classification.\\n* It is a type of layer.\"},{\"question_number\":\"vii\",\"question_text\":\"Fully connected layer\",\"answer_text\":\"* **Final decision making layer**.\\n* Each neuron connects to all output from previous layers.\"}]"
  },
  {
    "image": "IMG-20251026-WA0345.jpg",
    "ocr_text": "ii) Activation function: * It is used in the Artificial neural network for machine learning. * It plays a main role in machine learning & deep learning. i. Sigmoid function: * Sigmoid function is the first & main type in the activation function. * It determines whether a neuron should be activated or not. * Formula: $f(x) = y / (1 + e^{-x})$. * Output range: (0, 1). * Sigmoid function uses the binary classification, linear regression model. * It is a main type in activation function than other types. They are: * Tanh (Hyperbolic Tangent function) * Sigmoid * ReLu * Softmax",
    "json_output": "[{\"question_number\":\"ii\",\"question_text\":\"Activation function\",\"answer_text\":\"* It is used in the **Artificial neural network** for machine learning.\\n* It plays a main role in **machine learning & deep learning**.\"},{\"question_number\":\"i\",\"question_text\":\"Sigmoid function\",\"answer_text\":\"* **Sigmoid function** is the **first & main type** in the activation function.\\n* It determines whether a neuron should be **activated or not**.\\n* Formula: $f(x) = \\frac{y}{1 + e^{-x}}$ (Note: Standard formula is $f(x) = \\frac{1}{1 + e^{-x}}$).\\n* Output range: **(0, 1)**.\\n* **Sigmoid function** uses the **binary classification**, linear regression model.\\n* It is a main type in activation function.\"},{\"question_number\":\"undefined\",\"question_text\":\"Other Activation Types\",\"answer_text\":\"Other types:\\n* **Tanh (Hyperbolic Tangent function)**\\n* **Sigmoid**\\n* **ReLu**\\n* **Softmax**\"}]"
  },
  {
    "image": "IMG-20251026-WA0347.jpg",
    "ocr_text": "12. Building blocks of CNN: 1. Input layer: * Takes raw image data fed into the network. * A typical CNN architecture is made up of a sequence of layers: * For example: It is a main part of deep learning. There are four Building blocks in the convolutional neural network. * Must pass through the input layer to transform the input layer. 2. Convolutional layer. 3. Activation function. 4. Pooling layer. -> Flatten layer -> Fully connected layer -> Output layer.",
    "json_output": "[{\"question_number\":12,\"question_text\":\"Building blocks of CNN\",\"answer_text\":\"1. **Input layer**:\\n* Takes **raw image data** fed into the network.\\n* A typical CNN architecture is made up of a **sequence of layers**.\\n* It is a main part of **deep learning**.\\n* There are four Building blocks in the convolutional neural network (listed below).\\n* Must pass through the input layer to **transform the input layer**.\\n\\nSequence of layers:\\n2. **Convolutional layer**.\\n3. **Activation function**.\\n4. **Pooling layer**.\\n-> **Flatten layer**\\n-> **Fully connected layer**\\n-> **Output layer**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0346.jpg",
    "ocr_text": "2. Tanh (Hyperbolic Tangent function): * It is a proper type of Activation function. * The formula of tanh is: $\\text{Tanh}(x) = (e^x - e^{-x}) / (e^x + e^{-x})$. * Output range: (-1, 1). * It is universal. 4. ReLu: * Output range: [0, $\\infty$]. * $f(x) = \\max(0, x)$. 5. Softmax: * Softmax is also a main type of activation function in machine learning. * It gets the output into the range of 0 to 1. * It converts the sum up to 1. * It is mostly used for multiple classes. * Hidden layers generally use ReLu.",
    "json_output": "[{\"question_number\":2,\"question_text\":\"Tanh (Hyperbolic Tangent function)\",\"answer_text\":\"* It is a proper type of **Activation function**.\\n* Formula: $$ \\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\\n* Output range: **(-1, 1)**.\\n* It is **universal**.\"},{\"question_number\":4,\"question_text\":\"ReLu\",\"answer_text\":\"* Output range: $\\mathbf{[0, \\infty]}$ (0 to infinity).\\n* $f(x) = \\max(0, x)$.\"},{\"question_number\":5,\"question_text\":\"Softmax\",\"answer_text\":\"* **Softmax** is a main type of activation function in machine learning.\\n* It gets the output into the range of **0 to 1**.\\n* It converts the sum up to **1**.\\n* It is mostly used for **multiple classes**.\\n* **Hidden layers** generally use **ReLu**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0341.jpg",
    "ocr_text": "* update weights: $w = w - \\eta (\\frac{\\partial L}{\\partial w})$. * Advantages: - Learns complex patterns. - Used to model deep learning models. 12. Building blocks of CNN: i) Input layer: * Takes image data. * E.g.: 28x28 (grayscale), 64x64 (RGB).",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Weight Update and Advantages of NN/DL\",\"answer_text\":\"* **Weight update formula** (using gradient descent): $$ w = w - \\eta \\left(\\frac{\\partial L}{\\partial w}\\right) $$\\n* **Advantages**:\\n- Learns **complex patterns**.\\n- Used to model **deep learning models**.\"},{\"question_number\":12,\"question_text\":\"Building blocks of CNN (Input Layer)\",\"answer_text\":\"i) **Input layer**:\\n* Takes **image data**.\\n* E.g.: $\\mathbf{28 \\times 28}$ (grayscale), $\\mathbf{64 \\times 64}$ (RGB).\"}]"
  },
  {
    "image": "IMG-20251026-WA0344.jpg",
    "ocr_text": "vii) Output layer: * Produces prediction. * Often uses softmax for classification. [Diagram showing CNN Architecture: Input layer -> Dense layer -> Output layer] [Diagram showing CNN Architecture with Convolution: Input layer -> Convolution layer -> Dense layer -> Output layer]",
    "json_output": "[{\"question_number\":\"vii\",\"question_text\":\"Output layer\",\"answer_text\":\"* Produces **prediction**.\\n* Often uses **softmax** for **classification**.\"},{\"question_number\":\"undefined\",\"question_text\":\"CNN Architecture Diagrams\",\"answer_text\":\"[Diagram showing ANN/DNN Architecture: Input layer -> Dense layer -> Output layer]\\n[Diagram showing CNN Architecture with Convolution: Input layer -> Convolution layer -> Dense layer -> Output layer]\"}]"
  }
 {
    "image": "IMG-20251114-WA0015.jpg",
    "ocr_text": "Part C. Answer any two questions: 12) Convolutional neural network (CNN): * Convolutional neural network (CNN) makes efficient way of image processing and also makes accurate output. [Diagram showing Input layer -> Convolution -> Pooling -> Dense -> Output]. Input layer: The input image has accurate of large size and has overfilling. Thus the image is processed to make the image as per expected output size. (33x33x3).",
    "json_output": "[{\"question_number\":12,\"question_text\":\"Convolutional neural network (CNN)\",\"answer_text\":\"* **Convolutional neural network (CNN)** makes an **efficient way of image processing** and also makes **accurate output**.\\n[Diagram showing Input layer -> Convolution -> Pooling -> Dense -> Output].\\n\\n**Input layer**:\\n* The input image has an accurate, large size and has **overfilling**.\\n* Thus, the image is processed to make the image as per the expected output size, e.g., $\\mathbf{33 \\times 33 \\times 3}$.\"}]"
  },
  {
    "image": "IMG-20251114-WA0016.jpg",
    "ocr_text": "Convolution Layer: * Convolution layer takes the complex process of the image and process on it by using activation function. * It takes the edges and pixels of the image, showing the process. Pooling layer: * Pooling layer computes the image pixels and removes the unwanted edges on the image. $\\rightarrow (33 \\times 33 \\times 3)$. * This makes the image clean and execute for expected output. Dense layer: * After the pooling layer the image has to be processed by the output layer so it checks the image and processes the image clearly.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Convolution Layer\",\"answer_text\":\"* **Convolution layer** takes the complex process of the image and processes it by using the **activation function**.\\n* It takes the **edges and pixels** of the image, showing the process.\"},{\"question_number\":\"undefined\",\"question_text\":\"Pooling layer\",\"answer_text\":\"* **Pooling layer** computes the image pixels and **removes the unwanted edges** on the image.\\n* $\\rightarrow (33 \\times 33 \\times 3)$ (Output size example).\\n* This makes the image **clean** and executes for the expected output.\"},{\"question_number\":\"undefined\",\"question_text\":\"Dense layer and Output Layer\",\"answer_text\":\"* After the **pooling layer**, the image has to be processed by the **output layer** so it checks the image and processes the image **clearly**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0366.jpg",
    "ocr_text": "Layers in CNN are: i) Input layer: is the first layer in which the raw data or image is to process. ii) Convolution: In this layer the image is analyzed to the image to process the data to the pooling layer. It will show the edges, width and height. iii) Pooling layer: is used to reduce dimension and unwanted features in the image are passed on to the output. iv) Output: the output and the layer will be show.",
    "json_output": "[{\"question_number\":\"i\",\"question_text\":\"Input layer (CNN)\",\"answer_text\":\"* **Input layer** is the **first layer** in which the **raw data or image** is to process.\"},{\"question_number\":\"ii\",\"question_text\":\"Convolution Layer (CNN)\",\"answer_text\":\"* In this layer, the image is **analyzed** to process the data to the **pooling layer**.\\n* It will show the **edges, width and height**.\"},{\"question_number\":\"iii\",\"question_text\":\"Pooling layer (CNN)\",\"answer_text\":\"* **Pooling layer** is used to **reduce dimension** and **unwanted features** in the image are passed on to the output.\"},{\"question_number\":\"iv\",\"question_text\":\"Output Layer (CNN)\",\"answer_text\":\"* The **output** and the layer will be shown.\"}]"
  },
  {
    "image": "IMG-20251026-WA0365.jpg",
    "ocr_text": "$f(x) = \\max(0, x)$ [Graph showing ReLu function]. * Soft Max: It is used for **multiple classification problem**. * It takes the input and output a value between 0 and 1. [Graph showing Softmax function]. The building block of convolutional Neural network is a type of neural network which is used for image recognition and loss classification.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"ReLu and Softmax Activation Functions\",\"answer_text\":\"$$f(x) = \\max(0, x)$$ [Graph showing ReLu function].\\n\\n* **Soft Max**:\\nIt is used for **multiple classification problems**.\\nIt takes the input and outputs a value between **0 and 1**.\\n[Graph showing Softmax function].\\n\\n* The **building block of convolutional Neural network** is a type of neural network which is used for **image recognition** and **loss classification**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0372.jpg",
    "ocr_text": "Artificial Neural network works by using the weights and activation function. Formula: $\\sum_{i=1}^{n} (w_i x_i) + b$. [Diagram of a single perceptron]. Loss function: is the difference between the predicted value and the actual value. Two types of Loss functions: * Loss function for Regression. * Loss function for classification. Loss function for Regression: i) Mean Squared Error (MSE): * MSE is the most commonly used loss function in regression. * It measures the difference between actual and predicted value.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Artificial Neural Network Fundamentals\",\"answer_text\":\"**Artificial Neural network** works by using the **weights** and **activation function**.\\nFormula: $$ \\sum_{i=1}^{n} (w_i x_i) + b $$\\n[Diagram of a single perceptron].\\n\\n**Loss function**:\\n* Is the **difference between the predicted value and the actual value**.\\nTwo types of Loss functions:\\n* Loss function for **Regression**.\\n* Loss function for **classification**.\"},{\"question_number\":\"i\",\"question_text\":\"Mean Squared Error (MSE)\",\"answer_text\":\"* **MSE** is the **most commonly used loss function in regression**.\\n* It measures the **difference between actual and predicted value**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0364.jpg",
    "ocr_text": "[Graph showing Tanh function]. * Tanh function: It is the hyperbolic tangent function. Range: -1 to 1. Tanh $\\times 2 + \\text{Sigmoid} (2x) = 1$ (This formula is incorrect). $\\text{Tanh}(x) = (e^x - e^{-x}) / (e^x + e^{-x})$. [Graph showing ReLu function]. * ReLu: Gives output for positive value. $\\rightarrow$ Output 0 for negative value.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Tanh Activation Function\",\"answer_text\":\"[Graph showing Tanh function].\\n* **Tanh function** is the **hyperbolic tangent function**.\\n* Range: **-1 to 1**.\\n* $\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\"},{\"question_number\":\"undefined\",\"question_text\":\"ReLu Activation Function\",\"answer_text\":\"[Graph showing ReLu function].\\n* **ReLu** gives output for **positive value**.\\n* Gives **0** for **negative value**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0370.jpg",
    "ocr_text": "ii) Mean Absolute Error (MAE): * MAE is calculated by taking the mean value for the difference between actual and predicted value. $MAE = 1 / N \\sum |\\hat{y}_i - y_i|$. iii) Huber Loss: * Huber Loss is used for robust regression where it is sensitive to outliers. Formula is $\\delta$ (delta). Huber Loss is a combination of $L_{MSE}$ (for $|y_i - \\hat{y}_i| \\le \\delta$) and $L_{MAE}$ (for $|y_i - \\hat{y}_i| > \\delta$). Loss Function for classification: i) Binary cross entropy: It is used in binary classification. $\\text{Entropy} = - (y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i))$. ii) Categorical cross entropy: Used for multi-class classification.",
    "json_output": "[{\"question_number\":\"ii\",\"question_text\":\"Mean Absolute Error (MAE)\",\"answer_text\":\"* **MAE** is calculated by taking the **mean value** for the **difference between actual and predicted value**.\\n$$MAE = \\frac{1}{N} \\sum |\\hat{y}_i - y_i|$$\"},{\"question_number\":\"iii\",\"question_text\":\"Huber Loss\",\"answer_text\":\"* **Huber Loss** is used for **robust regression** where it is sensitive to outliers. It is a **combination of $L_{MSE}$ and $L_{MAE}$** based on a parameter $\\delta$.\"},{\"question_number\":\"i\",\"question_text\":\"Binary cross entropy (Classification)\",\"answer_text\":\"* It is used in **binary classification**.\\n$$ \\text{Entropy} = - \\left(y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i)\\right) $$.\"},{\"question_number\":\"ii\",\"question_text\":\"Categorical cross entropy (Classification)\",\"answer_text\":\"* Used for **multi-class classification**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0368.jpg",
    "ocr_text": "10. Neural Network: * Neural network is a mathematical model which trains to mimic the how human brain processes the data. * Artificial Neural network will train the model on its own without human intervention. * Artificial neural network is a supervised learning. * Artificial neural network gets the input image as the data and find the hidden patterns in data and gives the output. Perceptron: * Perceptron is a single layer neural network. It does not have any hidden layers. * It uses activation function to fine-tune the neuron. [Diagram of a single perceptron]. Formula for a perceptron: $y = f(\\sum (w_i x_i) + b)$. * Perceptron is used for binary solving problem. * Cannot handle complex problem. * Perceptron has Input layer along with weights. Activation function: will have a threshold value. If the neuron is activated or it is evolve, it will be activated. Multilayer Neural Network: * Multilayer Neural Network is an advancement of perceptron. * It is used to handle complex patterns. * It is a feedforward network.",
    "json_output": "[{\"question_number\":10,\"question_text\":\"Neural Network\",\"answer_text\":\"* **Neural network** is a **mathematical model** which trains to **mimic** how the **human brain processes the data**.\\n* **Artificial Neural network** trains the model on its own **without human intervention**.\\n* It is a **supervised learning** model.\\n* It gets the **input image** as data, finds the **hidden patterns** and gives the **output**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Perceptron\",\"answer_text\":\"* **Perceptron** is a **single layer neural network**.\\n* It **does not have any hidden layers**.\\n* It uses an **activation function** to fine-tune the neuron.\\n[Diagram of a single perceptron].\\nFormula for a perceptron: $y = f(\\sum (w_i x_i) + b)$.\\n* Perceptron is used for **binary solving problems**.\\n* Cannot handle **complex problems**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Activation Function\",\"answer_text\":\"* **Activation function** has a **threshold value**. If the neuron is activated, it evolves and will be activated.\"},{\"question_number\":\"undefined\",\"question_text\":\"Multilayer Neural Network\",\"answer_text\":\"* **Multilayer Neural Network** is an **advancement of perceptron**.\\n* It is used to handle **complex patterns**.\\n* It is a **feedforward network**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0367.jpg",
    "ocr_text": "[Diagram of a multi-layer perceptron: Input layer -> Hidden layer -> Output layer]. * It consists of: Input layer, Hidden layer, Output layer. * Input layer is the first layer where the inputs are passed into the network. * Hidden layer: It functions contains one or more hidden layers. * Hidden layer uses the activation function to pass the value from one layer to another. * Activation functions are ReLu, linear, Tanh, hard ReLu and sigmoid. * Output layer is the last layer where the output for useful input will be given. Back Propagation: * Back propagation is a core technique in Deep Learning that is used for network training with the neural network. * In back propagation, when the given output is error, this error will be back propagated to the input layer and the weights will be updated. * Forward pass: Calculating the prediction value using the known constant. * Loss calculation: The difference between predicted value and actual value will be measured (Error). * Back propagate: Error will back propagate from the output layer to input layer to modify the weights.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Neural Network Architecture and Layers\",\"answer_text\":\"[Diagram of a multi-layer perceptron: Input layer $\\rightarrow$ Hidden layer $\\rightarrow$ Output layer].\\n* It consists of: **Input layer, Hidden layer, Output layer**.\\n* **Input layer** is the **first layer** where inputs are passed into the network.\\n* **Hidden layer**: Contains **one or more hidden layers**. It uses the **activation function** to pass the value from one layer to another. Activation functions are **ReLu, linear, Tanh, hard ReLu, and sigmoid**.\\n* **Output layer** is the **last layer** where the **output** for useful input will be given.\\n* The output layer uses the activation function like **Sigmoid** or **Softmax**.\"},{\"question_number\":\"undefined\",\"question_text\":\"Back Propagation\",\"answer_text\":\"* **Back propagation** is a **core technique in Deep Learning** that is used for network training.\\n* When the given output is an **error**, this error will be **back propagated to the input layer** and the **weights will be updated**.\\n* **Forward pass**: Calculating the **prediction value** using the known constant.\\n* **Loss calculation**: The **difference between predicted value and actual value** will be measured (Error).\\n* **Back propagate**: **Error** will **back propagate** from the output layer to the input layer to **modify the weights**.\"}]"
  },
  {
    "image": "IMG-20251026-WA0363.jpg",
    "ocr_text": "[Graph showing Linear Activation function]. i) Linear activation function: * Linear activation function gives the output which is proportional to the input and will be the same output as input. Range $(-\\infty, \\infty)$. $y = x$. ii) Non-linear function: * Sigmoid function: * It is a S-shaped curve used for classification problem. * Range: 0 to 1. $f(x) = 1 / (1 + e^{-x})$.",
    "json_output": "[{\"question_number\":\"i\",\"question_text\":\"Linear Activation Function\",\"answer_text\":\"[Graph showing Linear Activation function].\\n* **Linear activation function** gives an output which is **proportional to the input** and will be the **same output as input**.\\n* Range $\\mathbf{(-\\infty, \\infty)}$. $y = x$.\"},{\"question_number\":\"ii\",\"question_text\":\"Non-linear function (Sigmoid)\",\"answer_text\":\"* **Sigmoid function**:\\n* It is an **S-shaped curve** used for **classification problem**.\\n* Range: **0 to 1**.\\n$$ f(x) = \\frac{1}{1 + e^{-x}} $$"}]"
  }
{
    "image": "IMG-20251114-WA0018.jpg",
    "ocr_text": "How the convolutional neural network (CNN) works in image processing. 17) Activation function: * Activation function is defined as a mathematical function which is used on every output of the neuron. It helps to decide whether the neuron output can be given or not. * It helps to define efficient output and helps us to know about the output techniques and how it works with the neural networks. * Function: It helps us to decide the correct and efficient output. Thus it gives output correctly and reduces the time taken by the output of the neural network.",
    "json_output": "[{\"question_number\":17,\"question_text\":\"Activation functions\",\"answer_text\":\"How the **convolutional neural network (CNN) works in image processing**.\\n\\n* **Activation function** is defined as a **mathematical function** which is used on every output of the neuron. It helps to decide whether the neuron output can be given or not.\\n* It helps to define an **efficient output** and helps us to know about the output techniques and how it works with the neural networks.\\n\\n**Function**:\\n* It helps us to decide the **correct and efficient output**.\\n* Thus, it gives output correctly and **reduces the time taken** by the output of the neural network.\"}]"
  },
  {
    "image": "IMG-20251114-WA0017.jpg",
    "ocr_text": "-> output layer: After all compressed layers, the image gets the process to execution which is the expected output of given image. So, the image process to be clean and having only unwanted things in the image. -> input image: 333x333x3. -> Convolution: Treat noise in the pixels. -> Pooling: Shrink the image by removing unwanted. [Matrix example for pooling] By 2x2, it is value [4, 8], [12, 16] when the image pixel read detailing.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Output Layer and Image Processing (CNN)\",\"answer_text\":\"-> **Output layer**:\\nAfter all compressed layers, the image gets processed to execution which is the **expected output** of the given image.\\nSo, the image process to be **clean** and having only **unwanted things removed** in the image.\\n\\n-> **Input image**: $333 \\times 333 \\times 3$. \\n\\n-> **Convolution**:\\nTreats **noise in the pixels**.\\n\\n-> **Pooling**:\\n**Shrink the image** by **removing unwanted**.\\n[Matrix example for pooling].\\nBy 2x2 pooling, the output is a value $[4, 8], [12, 16]$ (Max Pooling output example) when reading the image pixel detailing.\"}]"
  },
  {
    "image": "IMG-20251114-WA0020.jpg",
    "ocr_text": "Neural Network: * It receives input from previous layer. * The data process by layers to the output. * e.g.: watching the cat image which already stored, thus the neural network knows it is cat data to predict. Biological network: * It receives input from previous layer. * It takes the neural signal for the output. * e.g.: watching the cat image which already stored, thus the neural network knows it is cat data to predict. 7) Loss function: * Loss function helps us to study the loss experienced in between the actual and predicted. Loss function for regression: * Mean squared error: It determines the error and reduces the error by using the different calculation processes. $MSE = \\sum (y_{i} - \\hat{y}_{i})^2 / n$.",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Neural Network vs. Biological Network (Comparison)\",\"answer_text\":\"**Neural Network**:\\n* It receives input from the previous layer.\\n* The data is processed by layers to the output.\\n* **E.g.** The network identifies a stored cat image to predict/classify.\\n\\n**Biological network**:\\n* It receives input from the previous layer.\\n* It takes the **neural signal** for the output.\\n* **E.g.** Similar pattern recognition/storage as the NN example.\"},{\"question_number\":7,\"question_text\":\"Loss function (Regression)\",\"answer_text\":\"* **Loss function** helps us to study the **loss experienced in between the actual and predicted** values.\\n\\n**Loss function for regression**:\\n* **Mean squared error** (MSE):\\nIt determines the error and reduces the error by using different calculation processes.\\n$$MSE = \\frac{\\sum (y_{i} - \\hat{y}_{i})^2}{n}$$\"}]"
  },
  {
    "image": "IMG-20251114-WA0019.jpg",
    "ocr_text": "Linear function: * It is a type of function which is used as a simple to right complex models. * It helps us to study the changes of the gradient and its output. * It can be defined by a linear function. Eg: $f(x) = 2x + 3y + z$. [Graph showing a linear function].",
    "json_output": "[{\"question_number\":\"undefined\",\"question_text\":\"Linear Function / Linear Activation\",\"answer_text\":\"* It is a type of function which is used as a **simple to right complex models** (likely means 'to simplify complex models').\\n* It helps us to study the **changes of the gradient** and its output.\\n* It can be defined by a **linear function**.\\nE.g.: $f(x) = 2x + 3y + z$.\\n[Graph showing a linear function].\"}]"
  }
  